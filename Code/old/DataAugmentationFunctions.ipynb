{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "''' Author - Danish Gufran\n",
        "           - Jadon Johnson\n",
        "           - Isaac Blair\n",
        "           - Sudeep Pasricha\n",
        "\n",
        "EPIC Lab : Colorado State University, Fort Collins\n",
        "\n",
        "<Danish.Gufran@colostate.edu>\n",
        "<Jadon.Johnson@colostate.edu>\n",
        "<Isaac.Blair@colostate.edu>\n",
        "<Sudeep@colostate.edu>\n",
        "\n",
        "Title - Data Augmentation Methods for Wi-Fi RSS Fingerprinting Based Indoor Localization.\n",
        "\n",
        "Citation :\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "Urn8piKzSqxQ",
        "outputId": "d4997594-e7d3-4283-df34-64e956b7f823"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Author - Danish Gufran\\n           - Jadon Johnson\\n           - Isaac Blair\\n           - Sudeep Pasricha\\n<Danish.Gufran@colostate.edu>\\n<Jadon.Johnson@colostate.edu>\\n<isaac.blair@colostate.edu>\\n<sudeep@colostate.edu>\\n\\nData Augmentation Methods for Wi-Fi RSS Fingerprinting Based Indoor Localization.\\n\\nCitation : \\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT8GeBUZ_f9E",
        "outputId": "8a751db7-35fa-4702-b26d-412027db4f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'RSS_Database' already exists and is not an empty directory.\n",
            "fatal: destination path 'EPIC_Lab_Data' already exists and is not an empty directory.\n",
            "fatal: destination path 'heterogeneous-rssi-indoor-nav' already exists and is not an empty directory.\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.1)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n",
            "Requirement already satisfied: keras-multi-head in /usr/local/lib/python3.10/dist-packages (0.29.0)\n",
            "Requirement already satisfied: keras-self-attention==0.51.0 in /usr/local/lib/python3.10/dist-packages (from keras-multi-head) (0.51.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-self-attention==0.51.0->keras-multi-head) (1.26.4)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n"
          ]
        }
      ],
      "source": [
        "# !rm -rf maril\n",
        "try:\n",
        "  !git clone https://github.com/danishgufran/RSS_Database.git\n",
        "  !git clone https://github.com/danishgufran/EPIC_Lab_Data.git\n",
        "  !git clone https://github.com/EPIC-CSU/heterogeneous-rssi-indoor-nav.git\n",
        "  !pip install tensorflow-addons\n",
        "  !pip install keras-multi-head\n",
        "  !pip install catboost\n",
        "\n",
        "except:\n",
        "  from git import Repo  # pip install gitpython\n",
        "  Repo.clone_from(\"https://github.com/danishgufran/RSS_Database.git\")\n",
        "  Repo.clone_from(\"https://github.com/danishgufran/EPIC_Lab_Data.git\")\n",
        "  Repo.clone_from(\"https://github.com/EPIC-CSU/heterogeneous-rssi-indoor-nav.git\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import copy\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D , LSTM, Attention\n",
        "from tensorflow.keras.losses import *\n",
        "from tensorflow.keras.optimizers import*\n",
        "import random as random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "import pandas as pd\n",
        "\n",
        "import RSS_Database.Stone_Seth.Seth\n",
        "from RSS_Database.Stone_Seth.Seth import fetch_seth, Devices, Floorplan, get_mac_ids\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from EPIC_Lab_Data.data import Devices, Floorplan, build_dataset\n",
        "from EPIC_Lab_Data.helpers import compute_distances\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "import logging\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import Loss\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn import svm\n",
        "import xgboost as xgb\n",
        "\n",
        "from EPIC_Lab_Data.helpers import split_frame, compute_distances\n",
        "from EPIC_Lab_Data.data import build_dataset\n",
        "from EPIC_Lab_Data.Maril.MultiHeadAttentionAddon import MultiHeadAttentionAddon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixhfUhNmSynl",
        "outputId": "80468b14-189a-486c-b26b-88985c034e15"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_data(itr,dev, floorplan):\n",
        "    # dfs is a list of dataframes\n",
        "# meta is a dataframe with meta data\n",
        "\n",
        "#getting train data\n",
        "\n",
        "    train_fp, train_meta = fetch_seth(\n",
        "    dev,\n",
        "    str(floorplan),\n",
        "    ci = int(itr),\n",
        "    base_path=\"RSS_Database/Stone_Seth/temp/clean/\"  # <-- this would be 'seth/temp/clean' from outside this dir\n",
        ")\n",
        "    # train_fp, _, macs, lbl2cord = build_dataset(\n",
        "    #     dev,\n",
        "    #     str(floorplan),\n",
        "    # )\n",
        "    train_fp = train_fp.sample(frac=1).reset_index(drop=True)\n",
        "    train_aps = get_mac_ids(train_fp.columns)\n",
        "    train_x = train_fp[train_aps].values\n",
        "    # train_x = (train_x + 100)/100\n",
        "    train_y = (train_fp[\"label\"]).values\n",
        "    return train_x, train_y, train_aps\n",
        "\n",
        "def test_data(itr, train_aps, dev, floorplan):\n",
        "    #getting test data\n",
        "    test_fp, test_meta = fetch_seth(\n",
        "    dev ,\n",
        "    str(floorplan),\n",
        "    ci = itr,\n",
        "    base_path=\"RSS_Database/Stone_Seth/temp/clean/\"  # <-- this would be 'seth/temp/clean' from outside this dir\n",
        ")\n",
        "    # train_df, test_fp, macs_test, lbl2cords = build_dataset(\n",
        "    #       dev,\n",
        "    #       str(floorplan)\n",
        "    #   )\n",
        "    test_y = test_fp[\"label\"].values\n",
        "    # train_aps = train_aps.drop(['x', 'y','label'], axis=1)\n",
        "    # print(f'label -- {test_y}')\n",
        "    test_aps = get_mac_ids(test_fp.columns)\n",
        "    missing_aps = list(set(train_aps.columns)-set(test_aps))\n",
        "    test_fp[missing_aps] = 0\n",
        "\n",
        "    test_fp = test_fp.drop(['x', 'y','label'], axis=1)\n",
        "    test_x = test_fp[:]\n",
        "\n",
        "    # test_x = (test_x + 100)/100\n",
        "\n",
        "\n",
        "    return test_x, test_y\n",
        "\n",
        "def temp_train_data(dev, floorplan, ci_val):\n",
        "    # dfs is a list of dataframes\n",
        "# meta is a dataframe with meta data\n",
        "\n",
        "#getting train data\n",
        "\n",
        "    train_fp, train_meta = fetch_seth(\n",
        "    dev,\n",
        "    str(floorplan),\n",
        "    ci = ci_val,\n",
        "    base_path=\"RSS_Database/Stone_Seth/temp/clean/\"  # <-- this would be 'seth/temp/clean' from outside this dir\n",
        ")\n",
        "    # train_fp, _, macs, lbl2cord = build_dataset(\n",
        "    #     dev,\n",
        "    #     str(floorplan),\n",
        "    # )\n",
        "    train_fp = train_fp.sample(frac=1).reset_index(drop=True)\n",
        "    train_aps = get_mac_ids(train_fp.columns)\n",
        "    train_x = train_fp[train_aps].values\n",
        "    train_x = (train_x + 100)/100\n",
        "    train_y = (train_fp[\"label\"]).values\n",
        "    return train_x, train_y, train_aps\n",
        "def temp_test_data(train_aps, dev, floorplan, ci_val):\n",
        "    #getting test data\n",
        "    test_fp, test_meta = fetch_seth(\n",
        "    str(dev) ,\n",
        "    str(floorplan),\n",
        "    ci = ci_val,\n",
        "    base_path=\"RSS_Database/Stone_Seth/temp/clean/\"  # <-- this would be 'seth/temp/clean' from outside this dir\n",
        ")\n",
        "    # train_df, test_fp, macs_test, lbl2cords = build_dataset(\n",
        "    #       dev,\n",
        "    #       str(floorplan)\n",
        "    #   )\n",
        "    test_aps = get_mac_ids(test_fp.columns)\n",
        "    missing_aps = list(set(train_aps)-set(test_aps))\n",
        "    test_fp[missing_aps] = 0\n",
        "    test_x = test_fp[train_aps].values\n",
        "    test_x = (test_x + 100)/100\n",
        "    test_y = (test_fp[\"label\"]).values\n",
        "    return test_x, test_y"
      ],
      "metadata": {
        "id": "fZCKI2YL_5Pk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stacked_sae(input, train_x, train_y, eph):\n",
        "  print(\"\\n Training Stacked SAE... \\n\")\n",
        "  h1 = int(input - input*0.3)\n",
        "  h2 = int(h1 - h1*0.3)\n",
        "  h3 = int(h2 - h2*0.3)\n",
        "  h4 = int(h3 - h3*0.3)\n",
        "  #print(input,h1,h2,h3, h4)\n",
        "  # AE -1\n",
        "  input_img = keras.layers.Input(shape = (input, ))\n",
        "  GN = keras.layers.GaussianNoise(0.)(input_img)\n",
        "  distorted_input1 = Dropout(.1)(GN)\n",
        "  encoded1 = Dense(h1, activation = 'gelu')(distorted_input1)\n",
        "  decoded1 = Dense(input, activation = 'sigmoid')(encoded1)\n",
        "\n",
        "  autoencoder1 = keras.models.Model(inputs = input_img, outputs = decoded1)\n",
        "  encoder1 = keras.models.Model(inputs = input_img, outputs = encoded1)\n",
        "\n",
        "  # AE -2\n",
        "  encoded1_input = keras.layers.Input(shape = (h1,))\n",
        "  GN1 = keras.layers.GaussianNoise(0.)(encoded1_input)\n",
        "  distorted_input2 = Dropout(.10)(GN1)\n",
        "  encoded2 = Dense(h2, activation = 'gelu')(distorted_input2)\n",
        "  decoded2 = Dense(h1, activation = 'sigmoid')(encoded2)\n",
        "\n",
        "  autoencoder2 = tf.keras.Model(inputs = encoded1_input, outputs = decoded2)\n",
        "  encoder2 = tf.keras.Model(inputs = encoded1_input, outputs = encoded2)\n",
        "\n",
        "  # AE -3\n",
        "  encoded2_input = keras.layers.Input(shape = (h2,))\n",
        "  GN2 = keras.layers.GaussianNoise(0.)(encoded2_input)\n",
        "  distorted_input3 = Dropout(.10)(GN2)\n",
        "  encoded3 = Dense(h3, activation = 'gelu')(distorted_input3)\n",
        "  decoded3 = Dense(h2, activation = 'sigmoid')(encoded3)\n",
        "\n",
        "  autoencoder3 = tf.keras.Model(inputs = encoded2_input, outputs = decoded3)\n",
        "  encoder3 = tf.keras.Model(inputs = encoded2_input, outputs = encoded3)\n",
        "\n",
        "  # AE -4\n",
        "  encoded3_input = keras.layers.Input(shape = (h3,))\n",
        "  GN3 = keras.layers.GaussianNoise(0.)(encoded3_input)\n",
        "  distorted_input4 = Dropout(.10)(GN3)\n",
        "  encoded4 = Dense(h4, activation = 'gelu')(distorted_input4)\n",
        "  decoded4 = Dense(h3, activation = 'sigmoid')(encoded4)\n",
        "\n",
        "  autoencoder4 = tf.keras.Model(inputs = encoded3_input, outputs = decoded4)\n",
        "  encoder4 = tf.keras.Model(inputs = encoded3_input, outputs = encoded4)\n",
        "  # Final AE\n",
        "  encoded1_da = Dense(h1, activation = 'sigmoid')(input_img)\n",
        "  encoded2_da = Dense(h2, activation = 'sigmoid')(encoded1_da)\n",
        "  encoded3_da = Dense(h3, activation = 'sigmoid')(encoded2_da)\n",
        "  encoded4_da = Dense(h4, activation = 'sigmoid')(encoded3_da)\n",
        "  decoded4_da = Dense(h3, activation = 'sigmoid')(encoded4_da)\n",
        "  decoded3_da = Dense(h2, activation = 'sigmoid')(decoded4_da)\n",
        "  decoded2_da = Dense(h1, activation = 'sigmoid')(decoded3_da)\n",
        "  decoded1_da = Dense(input, activation = 'sigmoid')(decoded2_da)\n",
        "\n",
        "  deep_autoencoder = tf.keras.Model(inputs = input_img, outputs = decoded1_da)\n",
        "  #compile\n",
        "  sgd1 = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "  sgd2 = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "  sgd3 = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "  sgd4 = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "\n",
        "  sgdD = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "\n",
        "  autoencoder1.compile(loss='mse', optimizer = sgd1)\n",
        "  autoencoder2.compile(loss='mse', optimizer = sgd2)\n",
        "  autoencoder3.compile(loss='mse', optimizer = sgd3)\n",
        "  autoencoder4.compile(loss='mse', optimizer = sgd4)\n",
        "\n",
        "  encoder1.compile(loss='mse', optimizer = sgd1)\n",
        "  encoder2.compile(loss='mse', optimizer = sgd1)\n",
        "  encoder3.compile(loss='mse', optimizer = sgd1)\n",
        "  encoder4.compile(loss='mse', optimizer = sgd1)\n",
        "\n",
        "  deep_autoencoder.compile(loss='mse', optimizer = sgdD)\n",
        "  # fit ae 1\n",
        "  autoencoder1.fit(train_x, train_x,\n",
        "                epochs = eph,\n",
        "                validation_split = 0.0020,\n",
        "                shuffle = False, verbose =0,\n",
        "                callbacks=[\n",
        "                keras.callbacks.EarlyStopping(patience=200, restore_best_weights=True)])\n",
        "  first_layer_code = encoder1.predict(train_x)\n",
        "  #fit ae 2\n",
        "  autoencoder2.fit(first_layer_code, first_layer_code,\n",
        "                epochs = eph,\n",
        "                validation_split = 0.0020,\n",
        "                shuffle = False, verbose =0,\n",
        "                callbacks=[\n",
        "                keras.callbacks.EarlyStopping(patience=200, restore_best_weights=True)])\n",
        "  second_layer_code = encoder2.predict(first_layer_code)\n",
        "  #fit ae 3\n",
        "  autoencoder3.fit(second_layer_code, second_layer_code,\n",
        "               epochs = eph,\n",
        "               validation_split = 0.0020,\n",
        "               shuffle = False, verbose =0,\n",
        "                callbacks=[\n",
        "                keras.callbacks.EarlyStopping(patience=200, restore_best_weights=True)])\n",
        "  third_layer_code = encoder3.predict(second_layer_code)\n",
        "    #fit ae 4\n",
        "  autoencoder4.fit(third_layer_code, third_layer_code,\n",
        "               epochs = eph,\n",
        "               validation_split = 0.0020,\n",
        "               shuffle = False, verbose =0,\n",
        "                callbacks=[\n",
        "                keras.callbacks.EarlyStopping(patience=2000, restore_best_weights=True)])\n",
        "  fourth_layer_code = encoder4.predict(third_layer_code)\n",
        "  # Setting the weights of the deep autoencoder\n",
        "  deep_autoencoder.layers[1].set_weights(encoder1.layers[-1].get_weights()) # first dense layer\n",
        "\n",
        "  deep_autoencoder.layers[2].set_weights(encoder2.layers[-1].get_weights()) # second dense layer\n",
        "\n",
        "  deep_autoencoder.layers[3].set_weights(encoder3.layers[-1].get_weights()) # thrird dense layer\n",
        "\n",
        "  deep_autoencoder.layers[4].set_weights(encoder4.layers[-1].get_weights()) # fourth dense layer\n",
        "\n",
        "  # deep_autoencoder.layers[5].set_weights(autoencoder4.layers[-1].get_weights()) # fourth dense layer\n",
        "\n",
        "  deep_autoencoder.layers[6].set_weights(autoencoder3.layers[-1].get_weights()) # first decoder\n",
        "  deep_autoencoder.layers[7].set_weights(autoencoder2.layers[-1].get_weights()) # second decoder\n",
        "  deep_autoencoder.layers[8].set_weights(autoencoder1.layers[-1].get_weights()) # third decoder\n",
        "\n",
        "  deep_autoencoder.fit(train_x, train_x,\n",
        "                epochs = eph,\n",
        "                validation_split = 0.0020,\n",
        "                shuffle = False,\n",
        "                verbose =0,\n",
        "                callbacks=[keras.callbacks.EarlyStopping(patience=500, restore_best_weights=True)])\n",
        "\n",
        "  sae_fingerprint = []\n",
        "\n",
        "  for fingerprint in train_x:\n",
        "    fingerprint = fingerprint.reshape(1, -1)\n",
        "    sae_train_x = deep_autoencoder.predict(fingerprint, verbose = 0)\n",
        "    sae_train_x = np.array(sae_train_x)\n",
        "    rounded_data = np.round(sae_train_x[0], 2)  # Round to 2 decimal places\n",
        "    sae_fingerprint.append(rounded_data)\n",
        "\n",
        "  return np.array(sae_fingerprint), train_y\n"
      ],
      "metadata": {
        "id": "nplIyqc7wK6Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "import numpy as np\n",
        "\n",
        "# Generate augmented data using GAN\n",
        "\n",
        "def build_generator(noise_dim, output_dim):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(noise_dim, input_dim=noise_dim, activation='relu'))\n",
        "    # model.add(layers.Dense(64, activation='tanh'))\n",
        "    model.add(layers.Dense(int(noise_dim/4), activation='relu'))\n",
        "    model.add(layers.Dense(output_dim, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Discriminator Network\n",
        "def build_discriminator(input_dim):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128, input_dim=input_dim, activation='relu'))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    # model.add(layers.Dense(32, activation='tanh'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "def train_gan(train_x, train_y, noise_dim, batch_size, epochs):\n",
        "    input_dim = train_x.shape[1]\n",
        "    generator = build_generator(noise_dim, input_dim)\n",
        "    discriminator = build_discriminator(input_dim)\n",
        "\n",
        "    # Compile Discriminator\n",
        "    discriminator.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Combined model (Generator + Discriminator)\n",
        "    discriminator.trainable = False\n",
        "    gan_input = layers.Input(shape=(noise_dim,))\n",
        "    generated_data = generator(gan_input)\n",
        "    gan_output = discriminator(generated_data)\n",
        "    gan = models.Model(gan_input, gan_output)\n",
        "    gan.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy')\n",
        "\n",
        "    # Training the GAN\n",
        "    print(\"\\n Training GAN... \\n\")\n",
        "    for epoch in range(epochs):\n",
        "        for _ in range(len(train_x) // batch_size):\n",
        "            # Generate fake data\n",
        "            noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
        "            generated_data = generator.predict(noise, verbose=0)\n",
        "\n",
        "            # Select a random batch of real data\n",
        "            idx = np.random.randint(0, train_x.shape[0], batch_size)\n",
        "            real_data = train_x[idx]\n",
        "\n",
        "            # Labels for real and fake data\n",
        "            real_labels = np.ones((batch_size, 1))\n",
        "            fake_labels = np.zeros((batch_size, 1))\n",
        "\n",
        "            # Train the Discriminator\n",
        "            d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
        "            d_loss_fake = discriminator.train_on_batch(generated_data, fake_labels)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # Train the Generator\n",
        "            noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
        "            valid_y = np.ones((batch_size, 1))\n",
        "            g_loss = gan.train_on_batch(noise, valid_y)\n",
        "\n",
        "        if epoch :\n",
        "            print(f'Epoch {epoch}, D Loss: {d_loss[0]:.4f}, D Acc.: {100*d_loss[1]:.2f}%')\n",
        "\n",
        "    # Generate augmented data\n",
        "    gan_x = []\n",
        "    gan_y = []\n",
        "    for i in range(len(train_x)):\n",
        "        noise = np.random.normal(0, 1, (1, noise_dim))\n",
        "        generated_data = generator.predict(noise, verbose=0)\n",
        "        rounded_data = np.round(generated_data[0], 2)  # Round to 2 decimal places\n",
        "        gan_x.append(rounded_data)\n",
        "        gan_y.append(train_y[i])  # Assuming RP class remains the same\n",
        "\n",
        "    sorted_indices = np.argsort(train_y)\n",
        "    gan_x = np.array(gan_x)[sorted_indices]\n",
        "    gan_y = np.array(gan_y)[sorted_indices]\n",
        "    return gan_x, gan_y\n",
        "\n",
        "# Example usage\n",
        "\n",
        "# noise_dim = 100\n",
        "# batch_size = 64\n",
        "# epochs = 1000\n",
        "# gan_x, gan_y = train_gan(train_x, train_y, noise_dim, batch_size, epochs)\n"
      ],
      "metadata": {
        "id": "kCgHc1xea8e4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions Below :\n",
        "\n",
        "train_dev = ['OP3']\n",
        "\n",
        "dev = ['BLU','HTC','LG','MOTO','OP3','S7']\n",
        "\n",
        "floorplan = ['engr0']\n",
        "\n",
        "\n",
        "for train in train_dev:\n",
        "    for flp in floorplan:\n",
        "        print(f'\\n Train dev -> {train}   flp -> {flp} \\n')\n",
        "        # Initial CI only !!!\n",
        "        ci_val = 0\n",
        "        train_x, train_y, train_aps = temp_train_data(train, flp, ci_val)\n",
        "        sorted_indices = np.argsort(train_y)\n",
        "        train_x = train_x[sorted_indices]\n",
        "        train_y = train_y[sorted_indices]\n",
        "\n",
        "# Generate augmented data using GAN\n",
        "gan_x, gan_y = train_gan(train_x, train_y, noise_dim=int(train_x.shape[-1]), batch_size=int(train_x.shape[0]), epochs=100)\n",
        "\n",
        "# Generate augmented data using Stacked SAE\n",
        "stacked_sae_x, stacked_sae_y = stacked_sae(int(train_x.shape[-1]), train_x, train_y, 900)\n",
        "\n",
        "# Generate augmented data using SAE\n",
        "\n",
        "# Generate augmented data using FGSM\n",
        "\n",
        "# Generate augmented data using PGD\n",
        "\n",
        "# Generate augmented data using MIM\n",
        "\n",
        "print('Original RSS')\n",
        "print(train_x)\n",
        "print(\"\\n ---------------- \\n\")\n",
        "print('GAN RSS')\n",
        "print(gan_x)\n",
        "print(\"\\n ---------------- \\n\")\n",
        "print('Stacked SAE RSS')\n",
        "print(stacked_sae_x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6VgGIGYS7BG",
        "outputId": "a8493c34-ebff-4457-8c57-e452d75f229f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Train dev -> OP3   flp -> engr0 \n",
            "\n",
            "\n",
            " Training GAN... \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py:75: UserWarning: The model does not have any trainable weights.\n",
            "  warnings.warn(\"The model does not have any trainable weights.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, D Loss: 0.7732, D Acc.: 53.07%\n",
            "Epoch 2, D Loss: 0.7878, D Acc.: 50.69%\n",
            "Epoch 3, D Loss: 0.7999, D Acc.: 49.36%\n",
            "Epoch 4, D Loss: 0.8107, D Acc.: 48.31%\n",
            "Epoch 5, D Loss: 0.8209, D Acc.: 47.68%\n",
            "Epoch 6, D Loss: 0.8309, D Acc.: 47.36%\n",
            "Epoch 7, D Loss: 0.8414, D Acc.: 47.18%\n",
            "Epoch 8, D Loss: 0.8522, D Acc.: 46.92%\n",
            "Epoch 9, D Loss: 0.8632, D Acc.: 46.69%\n",
            "Epoch 10, D Loss: 0.8749, D Acc.: 46.59%\n",
            "Epoch 11, D Loss: 0.8870, D Acc.: 46.41%\n",
            "Epoch 12, D Loss: 0.8992, D Acc.: 46.37%\n",
            "Epoch 13, D Loss: 0.9118, D Acc.: 46.24%\n",
            "Epoch 14, D Loss: 0.9248, D Acc.: 46.19%\n",
            "Epoch 15, D Loss: 0.9383, D Acc.: 46.22%\n",
            "Epoch 16, D Loss: 0.9520, D Acc.: 46.17%\n",
            "Epoch 17, D Loss: 0.9659, D Acc.: 46.15%\n",
            "Epoch 18, D Loss: 0.9802, D Acc.: 46.14%\n",
            "Epoch 19, D Loss: 0.9945, D Acc.: 46.12%\n",
            "Epoch 20, D Loss: 1.0087, D Acc.: 46.09%\n",
            "Epoch 21, D Loss: 1.0229, D Acc.: 46.03%\n",
            "Epoch 22, D Loss: 1.0373, D Acc.: 45.98%\n",
            "Epoch 23, D Loss: 1.0517, D Acc.: 45.92%\n",
            "Epoch 24, D Loss: 1.0659, D Acc.: 45.90%\n",
            "Epoch 25, D Loss: 1.0800, D Acc.: 45.86%\n",
            "Epoch 26, D Loss: 1.0941, D Acc.: 45.82%\n",
            "Epoch 27, D Loss: 1.1082, D Acc.: 45.76%\n",
            "Epoch 28, D Loss: 1.1220, D Acc.: 45.73%\n",
            "Epoch 29, D Loss: 1.1356, D Acc.: 45.73%\n",
            "Epoch 30, D Loss: 1.1492, D Acc.: 45.74%\n",
            "Epoch 31, D Loss: 1.1626, D Acc.: 45.75%\n",
            "Epoch 32, D Loss: 1.1758, D Acc.: 45.77%\n",
            "Epoch 33, D Loss: 1.1887, D Acc.: 45.77%\n",
            "Epoch 34, D Loss: 1.2015, D Acc.: 45.74%\n",
            "Epoch 35, D Loss: 1.2141, D Acc.: 45.74%\n",
            "Epoch 36, D Loss: 1.2264, D Acc.: 45.77%\n",
            "Epoch 37, D Loss: 1.2385, D Acc.: 45.75%\n",
            "Epoch 38, D Loss: 1.2503, D Acc.: 45.75%\n",
            "Epoch 39, D Loss: 1.2618, D Acc.: 45.75%\n",
            "Epoch 40, D Loss: 1.2731, D Acc.: 45.76%\n",
            "Epoch 41, D Loss: 1.2842, D Acc.: 45.76%\n",
            "Epoch 42, D Loss: 1.2950, D Acc.: 45.78%\n",
            "Epoch 43, D Loss: 1.3056, D Acc.: 45.75%\n",
            "Epoch 44, D Loss: 1.3159, D Acc.: 45.77%\n",
            "Epoch 45, D Loss: 1.3260, D Acc.: 45.76%\n",
            "Epoch 46, D Loss: 1.3358, D Acc.: 45.73%\n",
            "Epoch 47, D Loss: 1.3453, D Acc.: 45.74%\n",
            "Epoch 48, D Loss: 1.3547, D Acc.: 45.73%\n",
            "Epoch 49, D Loss: 1.3639, D Acc.: 45.73%\n",
            "Epoch 50, D Loss: 1.3728, D Acc.: 45.74%\n",
            "Epoch 51, D Loss: 1.3816, D Acc.: 45.75%\n",
            "Epoch 52, D Loss: 1.3902, D Acc.: 45.73%\n",
            "Epoch 53, D Loss: 1.3986, D Acc.: 45.74%\n",
            "Epoch 54, D Loss: 1.4069, D Acc.: 45.74%\n",
            "Epoch 55, D Loss: 1.4150, D Acc.: 45.74%\n",
            "Epoch 56, D Loss: 1.4229, D Acc.: 45.74%\n",
            "Epoch 57, D Loss: 1.4307, D Acc.: 45.73%\n",
            "Epoch 58, D Loss: 1.4384, D Acc.: 45.72%\n",
            "Epoch 59, D Loss: 1.4459, D Acc.: 45.71%\n",
            "Epoch 60, D Loss: 1.4534, D Acc.: 45.72%\n",
            "Epoch 61, D Loss: 1.4607, D Acc.: 45.73%\n",
            "Epoch 62, D Loss: 1.4679, D Acc.: 45.73%\n",
            "Epoch 63, D Loss: 1.4750, D Acc.: 45.74%\n",
            "Epoch 64, D Loss: 1.4820, D Acc.: 45.72%\n",
            "Epoch 65, D Loss: 1.4888, D Acc.: 45.71%\n",
            "Epoch 66, D Loss: 1.4956, D Acc.: 45.71%\n",
            "Epoch 67, D Loss: 1.5022, D Acc.: 45.70%\n",
            "Epoch 68, D Loss: 1.5087, D Acc.: 45.69%\n",
            "Epoch 69, D Loss: 1.5151, D Acc.: 45.68%\n",
            "Epoch 70, D Loss: 1.5215, D Acc.: 45.68%\n",
            "Epoch 71, D Loss: 1.5277, D Acc.: 45.67%\n",
            "Epoch 72, D Loss: 1.5337, D Acc.: 45.67%\n",
            "Epoch 73, D Loss: 1.5397, D Acc.: 45.67%\n",
            "Epoch 74, D Loss: 1.5456, D Acc.: 45.67%\n",
            "Epoch 75, D Loss: 1.5514, D Acc.: 45.66%\n",
            "Epoch 76, D Loss: 1.5571, D Acc.: 45.65%\n",
            "Epoch 77, D Loss: 1.5626, D Acc.: 45.66%\n",
            "Epoch 78, D Loss: 1.5681, D Acc.: 45.65%\n",
            "Epoch 79, D Loss: 1.5735, D Acc.: 45.65%\n",
            "Epoch 80, D Loss: 1.5788, D Acc.: 45.65%\n",
            "Epoch 81, D Loss: 1.5840, D Acc.: 45.65%\n",
            "Epoch 82, D Loss: 1.5891, D Acc.: 45.64%\n",
            "Epoch 83, D Loss: 1.5942, D Acc.: 45.63%\n",
            "Epoch 84, D Loss: 1.5991, D Acc.: 45.62%\n",
            "Epoch 85, D Loss: 1.6039, D Acc.: 45.62%\n",
            "Epoch 86, D Loss: 1.6087, D Acc.: 45.61%\n",
            "Epoch 87, D Loss: 1.6133, D Acc.: 45.60%\n",
            "Epoch 88, D Loss: 1.6179, D Acc.: 45.60%\n",
            "Epoch 89, D Loss: 1.6224, D Acc.: 45.61%\n",
            "Epoch 90, D Loss: 1.6269, D Acc.: 45.61%\n",
            "Epoch 91, D Loss: 1.6312, D Acc.: 45.62%\n",
            "Epoch 92, D Loss: 1.6355, D Acc.: 45.61%\n",
            "Epoch 93, D Loss: 1.6396, D Acc.: 45.61%\n",
            "Epoch 94, D Loss: 1.6437, D Acc.: 45.60%\n",
            "Epoch 95, D Loss: 1.6478, D Acc.: 45.61%\n",
            "Epoch 96, D Loss: 1.6518, D Acc.: 45.61%\n",
            "Epoch 97, D Loss: 1.6557, D Acc.: 45.62%\n",
            "Epoch 98, D Loss: 1.6595, D Acc.: 45.61%\n",
            "Epoch 99, D Loss: 1.6632, D Acc.: 45.61%\n",
            "\n",
            " Training Stacked SAE... \n",
            "\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "Original RSS\n",
            "[[0.67 0.57 0.47 ... 0.   0.   0.  ]\n",
            " [0.71 0.57 0.45 ... 0.   0.   0.  ]\n",
            " [0.7  0.57 0.48 ... 0.   0.   0.  ]\n",
            " ...\n",
            " [0.59 0.   0.   ... 0.   0.   0.  ]\n",
            " [0.66 0.   0.   ... 0.18 0.   0.  ]\n",
            " [0.67 0.   0.   ... 0.   0.15 0.15]]\n",
            "\n",
            " ---------------- \n",
            "\n",
            "GAN RSS\n",
            "[[0.51 0.53 1.   ... 0.94 0.01 0.02]\n",
            " [0.35 0.51 1.   ... 0.99 0.   0.  ]\n",
            " [0.49 0.48 1.   ... 0.99 0.01 0.01]\n",
            " ...\n",
            " [0.4  0.53 1.   ... 0.97 0.01 0.01]\n",
            " [0.36 0.65 1.   ... 0.98 0.01 0.01]\n",
            " [0.36 0.63 1.   ... 0.98 0.01 0.01]]\n",
            "\n",
            " ---------------- \n",
            "\n",
            "Stacked SAE RSS\n",
            "[[0.68 0.52 0.46 ... 0.   0.   0.  ]\n",
            " [0.67 0.51 0.45 ... 0.   0.   0.  ]\n",
            " [0.67 0.52 0.45 ... 0.   0.   0.  ]\n",
            " ...\n",
            " [0.64 0.02 0.01 ... 0.03 0.04 0.05]\n",
            " [0.63 0.01 0.01 ... 0.04 0.04 0.07]\n",
            " [0.63 0.01 0.01 ... 0.05 0.04 0.08]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' Author - Danish Gufran\n",
        "           - Jadon Johnson\n",
        "           - Isaac Blair\n",
        "           - Sudeep Pasricha\n",
        "\n",
        "EPIC Lab : Colorado State University, Fort Collins\n",
        "\n",
        "<Danish.Gufran@colostate.edu>\n",
        "<Jadon.Johnson@colostate.edu>\n",
        "<Isaac.Blair@colostate.edu>\n",
        "<Sudeep@colostate.edu>\n",
        "\n",
        "Title - Data Augmentation Methods for Wi-Fi RSS Fingerprinting Based Indoor Localization.\n",
        "\n",
        "Citation :\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "8dHeQ1c2J3q3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "2ae44eb3-4f14-46b5-d64f-ab5bbe64187d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Author - Danish Gufran\\n           - Jadon Johnson\\n           - Isaac Blair\\n           - Sudeep Pasricha\\n<Danish.Gufran@colostate.edu>\\n<Jadon.Johnson@colostate.edu>\\n<isaac.blair@colostate.edu>\\n<sudeep@colostate.edu>\\n\\nTitle - Data Augmentation Methods for Wi-Fi RSS Fingerprinting Based Indoor Localization.\\n\\nCitation : \\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}