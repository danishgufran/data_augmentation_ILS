{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danishgufran/data_augmentation_ILS/blob/Dev/Code/DataAugmentationFunctions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "Urn8piKzSqxQ",
        "outputId": "e944bb53-5356-4c67-a0a3-18360bf216e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Author - Danish Gufran\\n           - Jadon Johnson\\n           - Isaac Blair\\n           - Sudeep Pasricha\\n\\nEPIC Lab : Colorado State University, Fort Collins\\n\\n<Danish.Gufran@colostate.edu>\\n<Jadon.Johnson@colostate.edu>\\n<Isaac.Blair@colostate.edu>\\n<Sudeep@colostate.edu>\\n\\nTitle - Data Augmentation Methods for Wi-Fi RSS Fingerprinting Based Indoor Localization.\\n\\nCitation :\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "''' Author - Danish Gufran\n",
        "           - Jadon Johnson\n",
        "           - Isaac Blair\n",
        "           - Sudeep Pasricha\n",
        "\n",
        "EPIC Lab : Colorado State University, Fort Collins\n",
        "\n",
        "<Danish.Gufran@colostate.edu>\n",
        "<Jadon.Johnson@colostate.edu>\n",
        "<Isaac.Blair@colostate.edu>\n",
        "<Sudeep@colostate.edu>\n",
        "\n",
        "Title - Data Augmentation Methods for Wi-Fi RSS Fingerprinting Based Indoor Localization.\n",
        "\n",
        "Citation :\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT8GeBUZ_f9E",
        "outputId": "f46295fa-625b-476d-aa08-c03447244f8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RSS_Database'...\n",
            "remote: Enumerating objects: 1521, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 1521 (delta 0), reused 5 (delta 0), pack-reused 1516 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1521/1521), 182.30 MiB | 19.31 MiB/s, done.\n",
            "Resolving deltas: 100% (738/738), done.\n",
            "Updating files: 100% (1655/1655), done.\n",
            "Cloning into 'EPIC_Lab_Data'...\n",
            "remote: Enumerating objects: 328, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 328 (delta 79), reused 105 (delta 77), pack-reused 221 (from 1)\u001b[K\n",
            "Receiving objects: 100% (328/328), 86.35 MiB | 23.70 MiB/s, done.\n",
            "Resolving deltas: 100% (162/162), done.\n",
            "Cloning into 'heterogeneous-rssi-indoor-nav'...\n",
            "remote: Enumerating objects: 80, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 80 (delta 1), reused 0 (delta 0), pack-reused 74 (from 1)\u001b[K\n",
            "Receiving objects: 100% (80/80), 4.07 MiB | 4.13 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.3.0\n",
            "    Uninstalling typeguard-4.3.0:\n",
            "      Successfully uninstalled typeguard-4.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.4.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n",
            "Collecting keras-multi-head\n",
            "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-self-attention==0.51.0 (from keras-multi-head)\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-self-attention==0.51.0->keras-multi-head) (1.26.4)\n",
            "Building wheels for collected packages: keras-multi-head, keras-self-attention\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14975 sha256=2e11e6a291a2633875a537c6e530158cd1bbebd499ebd283ea8ac8a1b308d802\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/23/4b/06d7ae21714f70fcc25b48f972cc8e5e7f4b6b764a038b509d\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18894 sha256=280db23c09ea83525efa63dfff51f0c961f17c81706e19a457486c50709540c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/f7/24/607b483144fb9c47b4ba2c5fba6b68e54aeee2d5bf6c05302e\n",
            "Successfully built keras-multi-head keras-self-attention\n",
            "Installing collected packages: keras-self-attention, keras-multi-head\n",
            "Successfully installed keras-multi-head-0.29.0 keras-self-attention-0.51.0\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n",
            "Collecting cleverhans\n",
            "  Downloading cleverhans-4.0.0-py3-none-any.whl.metadata (846 bytes)\n",
            "Collecting nose (from cleverhans)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting pycodestyle (from cleverhans)\n",
            "  Downloading pycodestyle-2.12.1-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from cleverhans) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from cleverhans) (3.7.1)\n",
            "Collecting mnist (from cleverhans)\n",
            "  Downloading mnist-0.2.2-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cleverhans) (1.26.4)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.10/dist-packages (from cleverhans) (0.24.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from cleverhans) (1.4.2)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.10/dist-packages (from cleverhans) (1.13)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from cleverhans) (1.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from cleverhans) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cleverhans) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cleverhans) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cleverhans) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cleverhans) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cleverhans) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cleverhans) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cleverhans) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cleverhans) (2.8.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->cleverhans) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->cleverhans) (2.2.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->cleverhans) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->cleverhans) (0.1.8)\n",
            "Downloading cleverhans-4.0.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.3/92.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycodestyle-2.12.1-py2.py3-none-any.whl (31 kB)\n",
            "Installing collected packages: nose, pycodestyle, mnist, cleverhans\n",
            "Successfully installed cleverhans-4.0.0 mnist-0.2.2 nose-1.3.7 pycodestyle-2.12.1\n"
          ]
        }
      ],
      "source": [
        "# !rm -rf maril\n",
        "try:\n",
        "  !git clone https://github.com/danishgufran/RSS_Database.git\n",
        "  !git clone https://github.com/danishgufran/EPIC_Lab_Data.git\n",
        "  !git clone https://github.com/EPIC-CSU/heterogeneous-rssi-indoor-nav.git\n",
        "  !pip install tensorflow-addons\n",
        "  !pip install keras-multi-head\n",
        "  !pip install catboost\n",
        "  !pip install cleverhans\n",
        "\n",
        "except:\n",
        "  from git import Repo  # pip install gitpython\n",
        "  Repo.clone_from(\"https://github.com/danishgufran/RSS_Database.git\")\n",
        "  Repo.clone_from(\"https://github.com/danishgufran/EPIC_Lab_Data.git\")\n",
        "  Repo.clone_from(\"https://github.com/EPIC-CSU/heterogeneous-rssi-indoor-nav.git\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ixhfUhNmSynl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "import io\n",
        "from contextlib import redirect_stdout\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import copy\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D , LSTM, Attention\n",
        "from tensorflow.keras.losses import *\n",
        "from tensorflow.keras.optimizers import*\n",
        "import random as random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "import pandas as pd\n",
        "\n",
        "import RSS_Database.Stone_Seth.Seth\n",
        "from RSS_Database.Stone_Seth.Seth import fetch_seth, Devices, Floorplan, get_mac_ids\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from EPIC_Lab_Data.data import Devices, Floorplan, build_dataset\n",
        "from EPIC_Lab_Data.helpers import compute_distances\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#import lightgbm as lgb\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "import numpy as np\n",
        "\n",
        "import logging\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import Loss\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn import svm\n",
        "import xgboost as xgb\n",
        "\n",
        "from EPIC_Lab_Data.helpers import split_frame, compute_distances\n",
        "from EPIC_Lab_Data.data import build_dataset\n",
        "from EPIC_Lab_Data.Maril.MultiHeadAttentionAddon import MultiHeadAttentionAddon\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method\n",
        "from cleverhans.tf2.attacks.projected_gradient_descent import projected_gradient_descent\n",
        "from cleverhans.tf2.attacks.momentum_iterative_method import momentum_iterative_method\n",
        "from cleverhans.tf2.attacks.carlini_wagner_l2 import carlini_wagner_l2\n",
        "from cleverhans.tf2.attacks.basic_iterative_method import basic_iterative_method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fZCKI2YL_5Pk"
      },
      "outputs": [],
      "source": [
        "def train_data(itr,dev, floorplan):\n",
        "    # dfs is a list of dataframes\n",
        "# meta is a dataframe with meta data\n",
        "\n",
        "#getting train data\n",
        "\n",
        "#     train_fp, train_meta = fetch_seth(\n",
        "#     dev,\n",
        "#     str(floorplan),\n",
        "#     ci = int(itr),\n",
        "#     base_path=\"RSS_DataEPIC_Lab_Data/Data/train/\"  # <-- this would be 'seth/temp/clean' from outside this dir\n",
        "# )\n",
        "    train_fp, _, macs, lbl2cord = build_dataset(\n",
        "        dev,\n",
        "        str(floorplan),\n",
        "    )\n",
        "    train_fp = train_fp.sample(frac=1).reset_index(drop=True)\n",
        "    train_aps = get_mac_ids(train_fp.columns)\n",
        "    train_x = train_fp[train_aps].values\n",
        "    #train_x = (train_x + 100)/100\n",
        "    train_y = (train_fp[\"label\"]).values\n",
        "    return train_x, train_y, train_aps\n",
        "\n",
        "def test_data(itr, train_aps, dev, floorplan):\n",
        "    #getting test data\n",
        "#     test_fp, test_meta = fetch_seth(\n",
        "#     dev ,\n",
        "#     str(floorplan),\n",
        "#     ci = itr,\n",
        "#     base_path=\"RSS_DataEPIC_Lab_Data/Data/test/\"  # <-- this would be 'seth/temp/clean' from outside this dir\n",
        "# )\n",
        "    train_df, test_fp, macs_test, lbl2cords = build_dataset(\n",
        "          dev,\n",
        "          str(floorplan)\n",
        "      )\n",
        "    test_aps = get_mac_ids(test_fp.columns)\n",
        "    missing_aps = list(set(train_aps)-set(test_aps))\n",
        "    missing_df = pd.DataFrame(0, index=test_fp.index, columns=missing_aps)\n",
        "    test_fp = pd.concat([test_fp, missing_df], axis=1)\n",
        "    test_x = test_fp[train_aps].values\n",
        "    #test_x = (test_x + 100)/100\n",
        "    test_y = (test_fp[\"label\"]).values\n",
        "\n",
        "    # test_x = (test_x + 100)/100\n",
        "\n",
        "\n",
        "    return np.array(test_x), test_y\n",
        "\n",
        "def temp_train_data(dev, floorplan, ci_val):\n",
        "    # dfs is a list of dataframes\n",
        "# meta is a dataframe with meta data\n",
        "\n",
        "#getting train data\n",
        "\n",
        "    train_fp, train_meta = fetch_seth(\n",
        "    dev,\n",
        "    str(floorplan),\n",
        "    ci = ci_val,\n",
        "    base_path=\"RSS_Database/Stone_Seth/temp/clean/\"  # <-- this would be 'seth/temp/clean' from outside this dir\n",
        ")\n",
        "    # train_fp, _, macs, lbl2cord = build_dataset(\n",
        "    #     dev,\n",
        "    #     str(floorplan),\n",
        "    # )\n",
        "    train_fp = train_fp.sample(frac=1).reset_index(drop=True)\n",
        "    train_aps = get_mac_ids(train_fp.columns)\n",
        "    train_x = train_fp[train_aps].values\n",
        "    train_x = (train_x + 100)/100\n",
        "    train_y = (train_fp[\"label\"]).values\n",
        "    return train_x, train_y, train_aps\n",
        "def temp_test_data(train_aps, dev, floorplan, ci_val):\n",
        "    #getting test data\n",
        "    test_fp, test_meta = fetch_seth(\n",
        "    str(dev) ,\n",
        "    str(floorplan),\n",
        "    ci = ci_val,\n",
        "    base_path=\"RSS_Database/Stone_Seth/temp/clean/\"  # <-- this would be 'seth/temp/clean' from outside this dir\n",
        ")\n",
        "    # train_df, test_fp, macs_test, lbl2cords = build_dataset(\n",
        "    #       dev,\n",
        "    #       str(floorplan)\n",
        "    #   )\n",
        "    test_aps = get_mac_ids(test_fp.columns)\n",
        "    missing_aps = list(set(train_aps)-set(test_aps))\n",
        "    test_fp[missing_aps] = 0\n",
        "    test_x = test_fp[train_aps].values\n",
        "    test_x = (test_x + 100)/100\n",
        "    test_y = (test_fp[\"label\"]).values\n",
        "    return test_x, test_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nplIyqc7wK6Y"
      },
      "outputs": [],
      "source": [
        "def stacked_sae(input, train_x, train_y, eph):\n",
        "  print(\"\\n Training Stacked SAE... \\n\")\n",
        "  h1 = int(input - input*0.3)\n",
        "  h2 = int(h1 - h1*0.3)\n",
        "  h3 = int(h2 - h2*0.3)\n",
        "  h4 = int(h3 - h3*0.3)\n",
        "  #print(input,h1,h2,h3, h4)\n",
        "  # AE -1\n",
        "  input_img = keras.layers.Input(shape = (input, ))\n",
        "  GN = keras.layers.GaussianNoise(0.)(input_img)\n",
        "  distorted_input1 = Dropout(.1)(GN)\n",
        "  encoded1 = Dense(h1, activation = 'gelu')(distorted_input1)\n",
        "  decoded1 = Dense(input, activation = 'sigmoid')(encoded1)\n",
        "\n",
        "  autoencoder1 = keras.models.Model(inputs = input_img, outputs = decoded1)\n",
        "  encoder1 = keras.models.Model(inputs = input_img, outputs = encoded1)\n",
        "\n",
        "  # AE -2\n",
        "  encoded1_input = keras.layers.Input(shape = (h1,))\n",
        "  GN1 = keras.layers.GaussianNoise(0.)(encoded1_input)\n",
        "  distorted_input2 = Dropout(.10)(GN1)\n",
        "  encoded2 = Dense(h2, activation = 'gelu')(distorted_input2)\n",
        "  decoded2 = Dense(h1, activation = 'sigmoid')(encoded2)\n",
        "\n",
        "  autoencoder2 = tf.keras.Model(inputs = encoded1_input, outputs = decoded2)\n",
        "  encoder2 = tf.keras.Model(inputs = encoded1_input, outputs = encoded2)\n",
        "\n",
        "  # AE -3\n",
        "  encoded2_input = keras.layers.Input(shape = (h2,))\n",
        "  GN2 = keras.layers.GaussianNoise(0.)(encoded2_input)\n",
        "  distorted_input3 = Dropout(.10)(GN2)\n",
        "  encoded3 = Dense(h3, activation = 'gelu')(distorted_input3)\n",
        "  decoded3 = Dense(h2, activation = 'sigmoid')(encoded3)\n",
        "\n",
        "  autoencoder3 = tf.keras.Model(inputs = encoded2_input, outputs = decoded3)\n",
        "  encoder3 = tf.keras.Model(inputs = encoded2_input, outputs = encoded3)\n",
        "\n",
        "  # AE -4\n",
        "  encoded3_input = keras.layers.Input(shape = (h3,))\n",
        "  GN3 = keras.layers.GaussianNoise(0.)(encoded3_input)\n",
        "  distorted_input4 = Dropout(.10)(GN3)\n",
        "  encoded4 = Dense(h4, activation = 'gelu')(distorted_input4)\n",
        "  decoded4 = Dense(h3, activation = 'sigmoid')(encoded4)\n",
        "\n",
        "  autoencoder4 = tf.keras.Model(inputs = encoded3_input, outputs = decoded4)\n",
        "  encoder4 = tf.keras.Model(inputs = encoded3_input, outputs = encoded4)\n",
        "  # Final AE\n",
        "  encoded1_da = Dense(h1, activation = 'sigmoid')(input_img)\n",
        "  encoded2_da = Dense(h2, activation = 'sigmoid')(encoded1_da)\n",
        "  encoded3_da = Dense(h3, activation = 'sigmoid')(encoded2_da)\n",
        "  encoded4_da = Dense(h4, activation = 'sigmoid')(encoded3_da)\n",
        "  decoded4_da = Dense(h3, activation = 'sigmoid')(encoded4_da)\n",
        "  decoded3_da = Dense(h2, activation = 'sigmoid')(decoded4_da)\n",
        "  decoded2_da = Dense(h1, activation = 'sigmoid')(decoded3_da)\n",
        "  decoded1_da = Dense(input, activation = 'sigmoid')(decoded2_da)\n",
        "\n",
        "  deep_autoencoder = tf.keras.Model(inputs = input_img, outputs = decoded1_da)\n",
        "  #compile\n",
        "  sgd1 = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "  sgd2 = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "  sgd3 = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "  sgd4 = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "\n",
        "  sgdD = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "\n",
        "  autoencoder1.compile(loss='mse', optimizer = sgd1)\n",
        "  autoencoder2.compile(loss='mse', optimizer = sgd2)\n",
        "  autoencoder3.compile(loss='mse', optimizer = sgd3)\n",
        "  autoencoder4.compile(loss='mse', optimizer = sgd4)\n",
        "\n",
        "  encoder1.compile(loss='mse', optimizer = sgd1)\n",
        "  encoder2.compile(loss='mse', optimizer = sgd1)\n",
        "  encoder3.compile(loss='mse', optimizer = sgd1)\n",
        "  encoder4.compile(loss='mse', optimizer = sgd1)\n",
        "\n",
        "  deep_autoencoder.compile(loss='mse', optimizer = sgdD)\n",
        "  # fit ae 1\n",
        "  autoencoder1.fit(train_x, train_x,\n",
        "                epochs = eph,\n",
        "                validation_split = 0.0020,\n",
        "                shuffle = False, verbose =0,\n",
        "                callbacks=[\n",
        "                keras.callbacks.EarlyStopping(patience=200, restore_best_weights=True)])\n",
        "  first_layer_code = encoder1.predict(train_x)\n",
        "  #fit ae 2\n",
        "  autoencoder2.fit(first_layer_code, first_layer_code,\n",
        "                epochs = eph,\n",
        "                validation_split = 0.0020,\n",
        "                shuffle = False, verbose =0,\n",
        "                callbacks=[\n",
        "                keras.callbacks.EarlyStopping(patience=200, restore_best_weights=True)])\n",
        "  second_layer_code = encoder2.predict(first_layer_code)\n",
        "  #fit ae 3\n",
        "  autoencoder3.fit(second_layer_code, second_layer_code,\n",
        "               epochs = eph,\n",
        "               validation_split = 0.0020,\n",
        "               shuffle = False, verbose =0,\n",
        "                callbacks=[\n",
        "                keras.callbacks.EarlyStopping(patience=200, restore_best_weights=True)])\n",
        "  third_layer_code = encoder3.predict(second_layer_code)\n",
        "    #fit ae 4\n",
        "  autoencoder4.fit(third_layer_code, third_layer_code,\n",
        "               epochs = eph,\n",
        "               validation_split = 0.0020,\n",
        "               shuffle = False, verbose =0,\n",
        "                callbacks=[\n",
        "                keras.callbacks.EarlyStopping(patience=2000, restore_best_weights=True)])\n",
        "  fourth_layer_code = encoder4.predict(third_layer_code)\n",
        "  # Setting the weights of the deep autoencoder\n",
        "  deep_autoencoder.layers[1].set_weights(encoder1.layers[-1].get_weights()) # first dense layer\n",
        "\n",
        "  deep_autoencoder.layers[2].set_weights(encoder2.layers[-1].get_weights()) # second dense layer\n",
        "\n",
        "  deep_autoencoder.layers[3].set_weights(encoder3.layers[-1].get_weights()) # thrird dense layer\n",
        "\n",
        "  deep_autoencoder.layers[4].set_weights(encoder4.layers[-1].get_weights()) # fourth dense layer\n",
        "\n",
        "  # deep_autoencoder.layers[5].set_weights(autoencoder4.layers[-1].get_weights()) # fourth dense layer\n",
        "\n",
        "  deep_autoencoder.layers[6].set_weights(autoencoder3.layers[-1].get_weights()) # first decoder\n",
        "  deep_autoencoder.layers[7].set_weights(autoencoder2.layers[-1].get_weights()) # second decoder\n",
        "  deep_autoencoder.layers[8].set_weights(autoencoder1.layers[-1].get_weights()) # third decoder\n",
        "\n",
        "  deep_autoencoder.fit(train_x, train_x,\n",
        "                epochs = eph,\n",
        "                validation_split = 0.0020,\n",
        "                shuffle = False,\n",
        "                verbose =0,\n",
        "                callbacks=[keras.callbacks.EarlyStopping(patience=500, restore_best_weights=True)])\n",
        "\n",
        "  sae_fingerprint = []\n",
        "\n",
        "  for fingerprint in train_x:\n",
        "    fingerprint = fingerprint.reshape(1, -1)\n",
        "    sae_train_x = deep_autoencoder.predict(fingerprint, verbose = 0)\n",
        "    sae_train_x = np.array(sae_train_x)\n",
        "    rounded_data = np.round(sae_train_x[0], 2)  # Round to 2 decimal places\n",
        "    sae_fingerprint.append(rounded_data)\n",
        "\n",
        "  return np.array(sae_fingerprint), train_y, deep_autoencoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "06D7GGvechAJ"
      },
      "outputs": [],
      "source": [
        "def sae(input_dim, train_x, train_y, epochs):\n",
        "    print(\"\\n Training Standard Autoencoder... \\n\")\n",
        "\n",
        "    # Define the architecture\n",
        "    h1 = int(input_dim - input_dim * 0.3)\n",
        "    h2 = int(h1 - h1 * 0.3)\n",
        "    h3 = int(h2 - h2 * 0.3)\n",
        "    h4 = int(h3 - h3 * 0.3)\n",
        "\n",
        "    # Input layer\n",
        "    input_img = keras.layers.Input(shape=(input_dim,))\n",
        "\n",
        "    # Encoder\n",
        "    encoded = Dense(h1, activation='gelu')(input_img)\n",
        "    encoded = Dense(h2, activation='gelu')(encoded)\n",
        "    encoded = Dense(h3, activation='gelu')(encoded)\n",
        "    encoded = Dense(h4, activation='gelu')(encoded)\n",
        "\n",
        "    # Decoder\n",
        "    decoded = Dense(h3, activation='gelu')(encoded)\n",
        "    decoded = Dense(h2, activation='gelu')(decoded)\n",
        "    decoded = Dense(h1, activation='gelu')(decoded)\n",
        "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "\n",
        "    # Autoencoder model\n",
        "    autoencoder = keras.models.Model(inputs=input_img, outputs=decoded)\n",
        "\n",
        "    # Compile the model\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train the autoencoder\n",
        "    autoencoder.fit(train_x, train_x,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=0.0020,\n",
        "                    shuffle=True,\n",
        "                    verbose=0,\n",
        "                    callbacks=[keras.callbacks.EarlyStopping(patience=200, restore_best_weights=True)])\n",
        "\n",
        "    # Encode the input data to generate the final feature representation\n",
        "    encoded_data = autoencoder.predict(train_x)\n",
        "\n",
        "    # Optionally round the encoded data\n",
        "    sae_fingerprint = np.round(encoded_data, 2)\n",
        "\n",
        "    return sae_fingerprint, train_y, autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kCgHc1xea8e4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Generate augmented data using GAN\n",
        "\n",
        "def build_generator(noise_dim, output_dim):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(noise_dim, input_dim=noise_dim, activation='relu'))\n",
        "    # model.add(layers.Dense(64, activation='tanh'))\n",
        "    model.add(layers.Dense(int(noise_dim), activation='relu'))\n",
        "    model.add(layers.Dense(output_dim, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Discriminator Network\n",
        "def build_discriminator(input_dim):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128, input_dim=input_dim, activation='relu'))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    # model.add(layers.Dense(32, activation='tanh'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "def train_gan(train_x, train_y, noise_dim, batch_size, epochs):\n",
        "    input_dim = train_x.shape[1]\n",
        "    generator = build_generator(noise_dim, input_dim)\n",
        "    discriminator = build_discriminator(input_dim)\n",
        "\n",
        "    # Compile Discriminator\n",
        "    discriminator.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Combined model (Generator + Discriminator)\n",
        "    discriminator.trainable = False\n",
        "    gan_input = layers.Input(shape=(noise_dim,))\n",
        "    generated_data = generator(gan_input)\n",
        "    gan_output = discriminator(generated_data)\n",
        "    gan = models.Model(gan_input, gan_output)\n",
        "    gan.compile(optimizer=optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy')\n",
        "\n",
        "    # Training the GAN\n",
        "    print(\"\\n Training GAN... \\n\")\n",
        "    for epoch in range(epochs):\n",
        "        for _ in range(len(train_x) // batch_size):\n",
        "            # Generate fake data\n",
        "            noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
        "            generated_data = generator.predict(noise, verbose=0)\n",
        "\n",
        "            # Select a random batch of real data\n",
        "            idx = np.random.randint(0, train_x.shape[0], batch_size)\n",
        "            real_data = train_x[idx]\n",
        "\n",
        "            # Labels for real and fake data\n",
        "            real_labels = np.ones((batch_size, 1))\n",
        "            fake_labels = np.zeros((batch_size, 1))\n",
        "\n",
        "            # Train the Discriminator\n",
        "            d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
        "            d_loss_fake = discriminator.train_on_batch(generated_data, fake_labels)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # Train the Generator\n",
        "            noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
        "            valid_y = np.ones((batch_size, 1))\n",
        "            g_loss = gan.train_on_batch(noise, valid_y)\n",
        "\n",
        "        # Uncomment below for training info (verbose = 1)\n",
        "        # if epoch :\n",
        "        #     print(f'Epoch {epoch}, D Loss: {d_loss[0]:.4f}, D Acc.: {100*d_loss[1]:.2f}%')\n",
        "\n",
        "    # Generate augmented data\n",
        "    gan_x = []\n",
        "    gan_y = []\n",
        "    for i in range(len(train_x)):\n",
        "        noise = np.random.normal(0, 1, (1, noise_dim))\n",
        "        generated_data = generator.predict(noise, verbose=0)\n",
        "        rounded_data = np.round(generated_data[0], 2)  # Round to 2 decimal places\n",
        "        gan_x.append(rounded_data)\n",
        "        gan_y.append(train_y[i])  # Assuming RP class remains the same\n",
        "\n",
        "    sorted_indices = np.argsort(train_y)\n",
        "    gan_x = np.array(gan_x)[sorted_indices]\n",
        "    gan_y = np.array(gan_y)[sorted_indices]\n",
        "    return gan_x, gan_y, generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lRHNuaXH4LbK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_fgsm(train_x, train_y, epsilon, model, aug, model_name, train, flp):\n",
        "    # Convert train_x and train_y to TensorFlow tensors\n",
        "    train_x = tf.convert_to_tensor(train_x, dtype=tf.float32)\n",
        "    train_y = tf.convert_to_tensor(train_y, dtype=tf.float32)\n",
        "\n",
        "    # Ensure model is compiled if not already\n",
        "    try:\n",
        "      if not model.compiled_loss:\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "    except:\n",
        "    #add safety DNN here\n",
        "      model = load_model(\"BackupDNN.keras\")\n",
        "      model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "    # Generate FGSM adversarial samples\n",
        "    fgsm_x = fast_gradient_method(model_fn=model, x=train_x, eps=epsilon, norm=np.inf)\n",
        "\n",
        "    return np.array(fgsm_x), train_y\n",
        "\n",
        "\n",
        "def train_pgd(train_x, train_y, epsilon, model, alpha=0.01, num_iterations=40):\n",
        "    # Convert train_x and train_y to TensorFlow tensors\n",
        "    train_x = tf.convert_to_tensor(train_x, dtype=tf.float32)\n",
        "    #train_y = tf.convert_to_tensor(train_x, dtype=tf.float32) <-- this is not needed and messes with dimensions of output\n",
        "\n",
        "    # Ensure model is compiled if not already\n",
        "    try:\n",
        "      if not model.compiled_loss:\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "    except:\n",
        "    #add safety DNN here\n",
        "      model = load_model(\"BackupDNN.keras\")\n",
        "      model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "    # Generate PGD adversarial samples\n",
        "    pgd_x = projected_gradient_descent(model_fn=model, x=train_x, eps=epsilon, eps_iter=alpha, nb_iter=num_iterations, norm=np.inf)\n",
        "\n",
        "    return np.array(pgd_x), train_y\n",
        "\n",
        "\n",
        "def train_mim(train_x, train_y, epsilon, model, alpha=0.01, num_iterations=40):\n",
        "    # Convert train_x and train_y to TensorFlow tensors\n",
        "    train_x = tf.convert_to_tensor(train_x, dtype=tf.float32)\n",
        "    #train_y = tf.convert_to_tensor(train_y, dtype=tf.float32)\n",
        "\n",
        "    # Ensure model is compiled if not already\n",
        "    try:\n",
        "      if not model.compiled_loss:\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "    except:\n",
        "    #add safety DNN here\n",
        "      model = load_model(\"BackupDNN.keras\")\n",
        "      model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "    # Generate MIM adversarial samples\n",
        "    mim_x = momentum_iterative_method(model_fn=model, x=train_x, eps=epsilon, eps_iter=alpha, nb_iter=num_iterations, norm=np.inf)\n",
        "\n",
        "    return np.array(mim_x), train_y\n",
        "\n",
        "def train_bim(train_x, train_y, epsilon, model, alpha=0.01, num_iterations=40):\n",
        "  # Convert train_x and train_y to TensorFlow tensors, this causes issues with catboost\n",
        "  train_x = tf.convert_to_tensor(train_x, dtype=tf.float32)\n",
        "  #train_y = tf.convert_to_tensor(train_y, dtype=tf.float32)\n",
        "\n",
        "  # Ensure model is compiled if not already\n",
        "  try:\n",
        "    if not model.compiled_loss:\n",
        "      model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "  except:\n",
        "  #add safety DNN here\n",
        "    model = load_model(\"BackupDNN.keras\")\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "  # Generate BIM adversarial samples\n",
        "  bim_x = basic_iterative_method(model_fn=model, x=train_x, eps=epsilon, eps_iter=alpha, nb_iter=num_iterations, norm=np.inf)\n",
        "\n",
        "  return np.array(bim_x), train_y\n",
        "\n",
        "def train_cw(train_x, train_y, epsilon, model):\n",
        "  # Convert train_x and train_y to TensorFlow tensors\n",
        "  train_x = tf.convert_to_tensor(train_x, dtype=tf.float32)\n",
        "  #train_y = tf.convert_to_tensor(train_y, dtype=tf.float32)\n",
        "\n",
        "  # Ensure model is compiled if not already\n",
        "  try:\n",
        "    if not model.compiled_loss:\n",
        "      model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "  except:\n",
        "  #add safety DNN here\n",
        "    model = load_model(\"BackupDNN.keras\")\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "  # Generate BIM adversarial samples\n",
        "  cw_x = carlini_wagner_l2(model_fn=model, x=train_x, clip_min = -100.0)\n",
        "\n",
        "  return np.array(cw_x), train_y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TmaxJOtloQeN"
      },
      "outputs": [],
      "source": [
        "def model_params(model, save_name):\n",
        "  # The summary can be commented to not print on console [Will still save in .txt]\n",
        "    print(f'\\n*** Model Summary : {save_name} ***\\n')\n",
        "    if isinstance(model, CatBoostClassifier):\n",
        "      params = model.get_all_params() #this is a build in CBC method, returns a dict\n",
        "      with open(f'{save_name}.txt', 'w') as f:\n",
        "        for key, value in params.items():\n",
        "            f.write(f'{key}: {value}\\n')\n",
        "        f.close()\n",
        "    else:\n",
        "      model.summary()\n",
        "      with open(f'{save_name}.txt', 'w') as f:\n",
        "          model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "          f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-1r8RIW6LPh1"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Add functions for the models here :\n",
        "\n",
        "# Add functions for :\n",
        "# KNN, GPC, SVM, RF, CatBoost, XgBoost, LightGBM, MLP, DNN, CNN, Attention, Transformer, VIT\n",
        "\n",
        "[1] Feel free to pass any arguments as input to the functions.\n",
        "[2] Each function must always return the trained model #return model\n",
        "[3] The DNN model is NOT optimized - you need to fix it\n",
        "[4] Play with the hyper-parameters to get the best trained model (you can change things as you seem fit)\n",
        "[5] You can use grid search methods like : NAS, Keras Tuner, Auto Keras and so on, to find the best hyper parameters\n",
        "    Note : It would be best if you open a new cell to test out your grid search method on the ML model and once you find the best model configuration you can insert that to the main run.\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "def train_DNN(train_x, train_y, ep = 50, batch_size=128):\n",
        "    print('Training DNN Classifier \\n')\n",
        "    # Build the DNN model\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(350, activation='gelu',input_shape=train_x.shape[1:]))\n",
        "\n",
        "    model.add(tf.keras.layers.GaussianNoise(0.15))\n",
        "    model.add(tf.keras.layers.Dropout(0.20))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(192, activation='gelu', kernel_regularizer=keras.regularizers.l2(0.01)))\n",
        "\n",
        "    model.add(tf.keras.layers.GaussianNoise(0.15))\n",
        "    model.add(tf.keras.layers.Dropout(0.20))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(128, activation='gelu', kernel_regularizer=keras.regularizers.l2(0.01)))\n",
        "\n",
        "    model.add(tf.keras.layers.GaussianNoise(0.15))\n",
        "    model.add(tf.keras.layers.Dropout(0.20))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(192, activation='gelu', kernel_regularizer=keras.regularizers.l2(0.01)))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(int(max(train_y)+50), activation='gelu'))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(int(max(train_y)+1), activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(train_x,train_y,\n",
        "                epochs = ep,\n",
        "                shuffle = True,\n",
        "                verbose = 0)\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_Catboost(train_x, train_y, iter):\n",
        "\n",
        "  model = CatBoostClassifier(\n",
        "      iterations = iter, # 1000 are ideal\n",
        "      loss_function='MultiClass',\n",
        "      bootstrap_type = \"Bayesian\",\n",
        "      eval_metric = 'MultiClass',\n",
        "      leaf_estimation_iterations = 100,\n",
        "      random_strength = 0.5,\n",
        "      depth = 7,\n",
        "      l2_leaf_reg = 5,\n",
        "      learning_rate=0.1,\n",
        "      bagging_temperature = 0.5,\n",
        "      task_type = \"GPU\",\n",
        "      silent = True\n",
        "  )\n",
        "  model.fit(train_x, train_y)\n",
        "\n",
        "  return model\n",
        "\n",
        "def train_XGboost(train_x, train_y):\n",
        "  model = XGBClassifier(\n",
        "      learning_rate = 0.3,\n",
        "      gamma\t= 1,\n",
        "      max_depth\t= 6,\n",
        "      colsample_bytree = 1,\n",
        "      reg_lambda =1,\n",
        "      n_estimators = 100,\n",
        "      scale_pos_weight = 1,\n",
        "      booster = \"gbtree\",\n",
        "      #tree_method = \"gpu_hist\",\n",
        "      subsample = 1\n",
        "      )\n",
        "  model.fit(train_x, train_y)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oui9V7qxwNs1"
      },
      "outputs": [],
      "source": [
        "''' Code Instructions :\n",
        "\n",
        "[1] Create new functions (in the cell above) to train all ML models (I have added research papers for each model for you to reference)\n",
        "        Please Note : The research papers are for reference to see how the particular model is trained. Look at the hyper-parameters the paper mentions and start with those, after which you can fine tune them.\n",
        "        These are the model -> KNN, GPC, SVM, RF, CatBoost, XgBoost, LightGBM, MLP, DNN, CNN, Attention, Transformer, VIT\n",
        "[2] Once the model functions are created call them in the cell below look out for '# [JJ] [IB] :' for additional information\n",
        "[3] Next update the model_name variable with the name of the new model\n",
        "[4] Use the new model name to add subsequent 'if' conditions for all training conditions\n",
        "[5] Now you need to fine tune the hyper-parameters in your new model to get the best trained model.\n",
        "    Here is how you can vaidate your model:\n",
        "    [5.A] Monitor the the accuracy and loss functions during training (accuracy must increase and loss must decrease)\n",
        "    [5.B] No need to change anything in SAE, Stacked SAE, GAN, FGSM, PGD, and MIM - Only update the new model functions\n",
        "    [5.C] Make sure to add all import and API calls in the 3rd cell of this notebook\n",
        "    [5.D] Make sure to add any pip installs (if needed) in the second cell of this notebook\n",
        "    [5.E] Finally, check the summary (CSV) files that gets generated after inference to see the average error for each case, ideally we need lower errors for all or MOST cases\n",
        "          One way to evaluate if your model is trained well is to see if it atleast (pay attention here) has lower errors for the conditions it was trained on\n",
        "          E.g, If the model was trained only on ORIGINAL data it must show low errors for inference on original data (across all devices) : This is the bare minimum, if other inferences like FGSM and so on show low errors as well then you have a great model (You need to aim to get this)\n",
        "          E.g, If the model was trained only on ORIGINAL + FGSM data it must show low errors for inference on original and FGSM data (across all devices)\n",
        "    [5.F] At the end you will notice that the script downloads a .zip file with the trained model (.keras), model summary (.txt), and inference output (.csv) - also another .csv file with the summary (average errors across all inference conditions). You will report that .zip (assuming with the best hyper-parameters of the model) to me.\n",
        "[6] Before you submit the .zip file and the updated .ipynb (with your additions to the ML model) make sure your team sits together and merge all changes into 1 .ipynb (so that i have the most recent notebook with changes from everyone)\n",
        "    [6.A] During the merge make sure the script has no errors and the results are satisfactory.\n",
        "    [6.B] Take this oppurtunity to evaluate each others changes to check if the results are satisfactory. Only report data to me when both of you are convinced with the results. I will do one last check before we move on to Phase 4 (As discussed in our meetings).\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeoT9hUT7DI3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Here are some research papers for you to get started with. Look at the hyper-parameters the paper proposes and start with those and then use other sources (internet) to learn the uses of different hyper-parameters and start experimenting on them.\n",
        "You can also create a temporary cell somewhere below to test your model before you run it in the main cell.\n",
        "The main cell may run for a couple of hours, so make sure there are no errors as you go.\n",
        "\n",
        "Here are the list of research papers:\n",
        "\n",
        "KNN [1], GPC [2], SVM [3], RF [4], CatBoost [5], XgBoost [6], LightGBM [7], MLP [8], DNN [9], CNN [10], Attention [11], Transformer [12], VIT [13]\n",
        "\n",
        "[1] KNN : https://ieeexplore.ieee.org/abstract/document/7386596\n",
        "\n",
        "[2] GPC : https://ieeexplore.ieee.org/abstract/document/8767421\n",
        "\n",
        "[3] SVM : https://ieeexplore.ieee.org/abstract/document/7986446\n",
        "\n",
        "[4] RF : https://link.springer.com/article/10.1007/s11277-020-07977-w\n",
        "\n",
        "[5] CatBoost : https://ieeexplore.ieee.org/abstract/document/10130598\n",
        "\n",
        "[6] XgBoost : https://www.mdpi.com/1424-8220/22/17/6629\n",
        "\n",
        "[7] LightGBM : https://www.mdpi.com/1424-8220/21/8/2722\n",
        "\n",
        "[8] MLP : https://ieeexplore.ieee.org/abstract/document/9606541\n",
        "\n",
        "[9] DNN : https://dl.acm.org/doi/full/10.1145/3607919\n",
        "\n",
        "[10] CNN : https://ieeexplore.ieee.org/abstract/document/9060340\n",
        "\n",
        "[11] Attention : https://ieeexplore.ieee.org/abstract/document/9918120\n",
        "\n",
        "[12] Transformer : https://ieeexplore.ieee.org/abstract/document/9923937\n",
        "\n",
        "[13] VIT : https://ieeexplore.ieee.org/abstract/document/10247684\n",
        "\n",
        "[13] VIT : https://arxiv.org/abs/2010.11929\n",
        "'''\n",
        "\n",
        "# NAS, Keras tuner, auto keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tR2SJNU56ve"
      },
      "outputs": [],
      "source": [
        "from keras.regularizers import l2\n",
        "\n",
        "train_x, train_y, train_aps = train_data(0,\"OP3\", 'engr0')\n",
        "train = 'OP3'\n",
        "flp = 'engr0'\n",
        "train_ap_dict = {}\n",
        "train_ap_dict[f'{train}_{flp}'] = train_aps\n",
        "sorted_indices = np.argsort(train_y)\n",
        "train_x = train_x[sorted_indices]\n",
        "train_y = train_y[sorted_indices]\n",
        "\n",
        "#model = train_DNN(train_x, train_y, 300, batch_size=128)\n",
        "print('Training DNN Classifier \\n')\n",
        "    # Build the DNN model\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(350, activation='gelu', input_shape=train_x.shape[1:], kernel_regularizer=l2(0.001)),\n",
        "    layers.Dropout(0.2),\n",
        "    tf.keras.layers.BatchNormalization(center = True, scale = False),\n",
        "\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    #layers.Dropout(0.2),\n",
        "    tf.keras.layers.BatchNormalization(center = True, scale = False),\n",
        "\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    #layers.Dropout(0.2),\n",
        "    tf.keras.layers.BatchNormalization(center = True, scale = False),\n",
        "\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    tf.keras.layers.BatchNormalization(center = True, scale = False),\n",
        "\n",
        "    layers.Dense(int(max(train_y)+1), activation='sigmoid'),\n",
        "])\n",
        "#model.add(tf.keras.layers.Dense(350, activation='relu',input_shape=train_x.shape[1:]))\n",
        "#model.add(tf.keras.layers.Dense(int(max(train_y)+1), activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate = 0.005),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "history = model.fit(train_x,train_y, epochs = 600,shuffle = True, verbose = 0)\n",
        "print(history.history['accuracy'])\n",
        "print(history.history['loss'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Z80jB0WLvnYR"
      },
      "outputs": [],
      "source": [
        "train_x, train_y, train_aps = train_data(0,\"OP3\", 'engr0')\n",
        "train = 'OP3'\n",
        "flp = 'engr0'\n",
        "train_ap_dict = {}\n",
        "train_ap_dict[f'{train}_{flp}'] = train_aps\n",
        "sorted_indices = np.argsort(train_y)\n",
        "#train_x = train_x[sorted_indices]\n",
        "#train_y = train_y[sorted_indices]\n",
        "\n",
        "model = train_Catboost(train_x, train_y, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZVqBe-7rBJD",
        "outputId": "c8ad85f4-0a5d-4baa-dcff-7d5e6c3b7551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_error:\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "prediction_classes:\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
            " 48 49 50 51 52 53 54 55 56 57 58 59 60]\n",
            "(61,)\n",
            "(61,)\n"
          ]
        }
      ],
      "source": [
        "ci_val = 0\n",
        "train_aps_value = train_ap_dict[f'{train}_{flp}']\n",
        "test_x, test_y = test_data(ci_val, train_aps_value, train, flp)\n",
        "predictions = model.predict(test_x)\n",
        "pred_class = []\n",
        "for prediction in predictions:\n",
        "  pred_class.append(prediction[0])\n",
        "pred_class = np.array(pred_class)\n",
        "\n",
        "prediction_classes = np.argmax(predictions, axis=-1)\n",
        "\n",
        "mean_error = np.abs(pred_class - test_y)\n",
        "print(\"mean_error:\")\n",
        "print(mean_error)\n",
        "print(\"prediction_classes:\")\n",
        "print(pred_class)\n",
        "\n",
        "#print(predictions[0].shape)\n",
        "\n",
        "print(pred_class.shape)\n",
        "print(test_y.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "collapsed": true,
        "id": "o6VgGIGYS7BG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "7ea02363-3d2b-4e2b-b0b4-cc91793e09a2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-b94f604a59e0>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'CATBOOST'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m           \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_Catboost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgan_org_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_org_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m           \u001b[0msave_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{aug}_{model_name}_eps_0_train_{train}_{flp}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{save_name}.cbm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-e09702b0a255>\u001b[0m in \u001b[0;36mtrain_Catboost\u001b[0;34m(train_x, train_y, iter)\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msilent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   )\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5243\u001b[0m             \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_function'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5245\u001b[0;31m         self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[0m\u001b[1;32m   5246\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5247\u001b[0m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mplot_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training plots'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m                 self._train(\n\u001b[0m\u001b[1;32m   2411\u001b[0m                     \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m                     \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_sets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "# KNN, GPC, SVM, RF, CatBoost, XgBoost, LightGBM, MLP, DNN, CNN, Attention, Transformer, VIT\n",
        "\n",
        "# [JJ] [IB] : Make sure to update this variable while running other models\n",
        "# Define the ML model to train\n",
        "model_name = 'DNN' #'CATBOOST' #'XGBOOST' #'DNN'\n",
        "\n",
        "# Training Device\n",
        "train = 'OP3'\n",
        "\n",
        "# Testing Device\n",
        "dev = ['BLU','HTC','LG','MOTO','OP3','S7', 'i12p', 'nk7', 'pxl4']\n",
        "\n",
        "# Building Floorplans\n",
        "# ['engr0', 'engr1', 'glover', 'sciences', 'libstudy']\n",
        "floorplan = ['engr0', 'engr1', 'glover', 'sciences', 'libstudy']\n",
        "\n",
        "# Different augmentations to train the ML model\n",
        "augmented_data = ['ORIGINAL','SAE', 'STACKED_SAE', 'GAN', 'FGSM', 'PGD', 'MIM', 'BIM']\n",
        "\n",
        "mode = ['ORIGINAL', 'SAE', 'STACKED_SAE', 'GAN']\n",
        "\n",
        "# Test the trained model on different conditions\n",
        "attack = ['ORIGINAL', 'FGSM', 'PGD', 'MIM', 'BIM']\n",
        "\n",
        "# Perturbation strength for Adversarial Methods (FGSM, PGD, MIM)\n",
        "# [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "epsilon = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "\n",
        "train_ap_dict = {}\n",
        "\n",
        "for flp in floorplan:\n",
        "    print(f'\\n Train dev -> {train}   flp -> {flp} \\n')\n",
        "    # Initial CI only !!!\n",
        "    ci_val = 0\n",
        "    train_x, train_y, train_aps = train_data(ci_val,train, flp)\n",
        "    train_ap_dict[f'{train}_{flp}'] = train_aps\n",
        "    sorted_indices = np.argsort(train_y)\n",
        "    train_x = train_x[sorted_indices]\n",
        "    train_y = train_y[sorted_indices]\n",
        "\n",
        "    for aug in augmented_data:\n",
        "      if aug == 'ORIGINAL':\n",
        "        print(f'Training on Original RSS - Train Device : {train}  Floorplan : {flp} \\n')\n",
        "\n",
        "        print(f'\\n************ Training Model : {aug} {flp} + {model_name} ************\\n')\n",
        "\n",
        "        # [JJ] [IB] : Add more if conditions for other models\n",
        "        if model_name == 'DNN':\n",
        "          model = train_DNN(train_x, train_y, 700, batch_size=128)\n",
        "          save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "          model.save(f'{save_name}.keras')\n",
        "          model.save(\"BackupDNN.keras\")\n",
        "          model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "\n",
        "        print(f'Training ORIGINAL Complete .... \\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "        if model_name == 'CATBOOST':\n",
        "          model = train_Catboost(train_x, train_y, 50)\n",
        "          save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "          model.save_model(f'{save_name}.cbm')\n",
        "          model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "\n",
        "        print(f'Training ORIGINAL Complete .... \\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "        if model_name == 'XGBOOST':\n",
        "          model = train_XGboost(train_x, train_y)\n",
        "          save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "          model.save_model(f'{save_name}.json')\n",
        "          #model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "\n",
        "        print(f'Training ORIGINAL Complete .... \\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "      if aug == 'SAE':\n",
        "        print(f'Training on SAE  - Train Device : {train}  Floorplan : {flp} \\n')\n",
        "        # Replace this with other models to train\n",
        "        sae_x, sae_y, sae_model = sae(int(train_x.shape[-1]), train_x, train_y, 350)\n",
        "        model_params(sae_model, f'{aug}_{train}_{flp}')\n",
        "        sae_org_x = np.concatenate((train_x, sae_x), axis=0)\n",
        "        sae_org_y = np.concatenate((train_y, sae_y), axis=0)\n",
        "        print(f'\\n************ Training Model : {aug} {flp} + {model_name} ************\\n')\n",
        "\n",
        "        # [JJ] [IB] : Add more if conditions for other models\n",
        "        if model_name == 'DNN':\n",
        "          model = train_DNN(sae_org_x, sae_org_y, 700, batch_size=128)\n",
        "          save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "          model.save(f'{save_name}.keras')\n",
        "          # model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "\n",
        "        print(f'Training SAE Complete .... \\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "        if model_name == 'CATBOOST':\n",
        "          model = train_Catboost(sae_org_x, sae_org_y, 50)\n",
        "          save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "          model.save_model(f'{save_name}.cbm')\n",
        "          model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "\n",
        "        print(f'Training SAE Complete .... \\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "        if model_name == 'XGBOOST':\n",
        "          model = train_XGboost(sae_org_x, sae_org_y)\n",
        "          save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "          model.save_model(f'{save_name}.json')\n",
        "          #model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "\n",
        "        print(f'Training SAE Complete .... \\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "      if aug == 'STACKED_SAE':\n",
        "        print(f'Training on Stacked SAE - Train Device : {train}  Floorplan : {flp} \\n')\n",
        "        # Replace this with other models to train\n",
        "        stacked_sae_x, stacked_sae_y, stacked_sae_model = stacked_sae(int(train_x.shape[-1]), train_x, train_y, 300)\n",
        "        model_params(stacked_sae_model, f'{aug}_{train}_{flp}')\n",
        "        sta_sae_org_x = np.concatenate((train_x, stacked_sae_x), axis=0)\n",
        "        sta_sae_org_y = np.concatenate((train_y, stacked_sae_y), axis=0)\n",
        "        print(f'\\n************ Training Model : {aug} {flp} + {model_name} ************\\n')\n",
        "\n",
        "        # [JJ] [IB] : Add more if conditions for other models\n",
        "        if model_name == 'DNN':\n",
        "          model = train_DNN(sta_sae_org_x, sta_sae_org_y, 700, batch_size=128)\n",
        "          save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "          model.save(f'{save_name}.keras')\n",
        "          # model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "\n",
        "        print(f'Training STACKED SAE Complete .... \\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "        if model_name == 'CATBOOST':\n",
        "          model = train_Catboost(sta_sae_org_x, sta_sae_org_y, 50)\n",
        "          save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "          model.save_model(f'{save_name}.cbm')\n",
        "          model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "\n",
        "        print(f'Training STACKED SAE Complete .... \\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "        if model_name == 'XGBOOST':\n",
        "          model = train_XGboost(sta_sae_org_x, sta_sae_org_y)\n",
        "          save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "          model.save_model(f'{save_name}.json')\n",
        "          #model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "\n",
        "        print(f'Training STACKED SAE Complete .... \\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "      if aug == 'GAN':\n",
        "        print(f'Training on GAN - Train Device : {train}  Floorplan : {flp} \\n')\n",
        "        # Replace this with other models to train\n",
        "        gan_x, gan_y, gan_model = train_gan(train_x, train_y, noise_dim=int(train_x.shape[-1]), batch_size=int(train_x.shape[0]), epochs=20)\n",
        "        model_params(gan_model, f'{aug}_{train}_{flp}')\n",
        "        gan_org_x = np.concatenate((train_x, gan_x), axis=0)\n",
        "        gan_org_y = np.concatenate((train_y, gan_y), axis=0)\n",
        "        print(f'\\n************ Training Model : {aug} {flp} + {model_name} ************\\n')\n",
        "\n",
        "        # [JJ] [IB] : Add more if conditions for other models\n",
        "        if model_name == 'DNN':\n",
        "          model = train_DNN(gan_org_x, gan_org_y, 700, batch_size=128)\n",
        "          save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "          model.save(f'{save_name}.keras')\n",
        "          model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "\n",
        "        print(f'Training GAN Complete .... \\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "        if model_name == 'CATBOOST':\n",
        "          model = train_Catboost(gan_org_x, gan_org_y, 50)\n",
        "          save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "          model.save_model(f'{save_name}.cbm')\n",
        "          model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "\n",
        "        print(f'Training GAN Complete .... \\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "        if model_name == 'XGBOOST':\n",
        "          model = train_XGboost(gan_org_x, gan_org_y)\n",
        "          save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "          model.save_model(f'{save_name}.json')\n",
        "          #model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "\n",
        "        print(f'Training GAN Complete .... \\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "      if aug == 'FGSM':\n",
        "        for eps in epsilon:\n",
        "          print(f'Training on FGSM with Epsilon = {eps} - Train Device : {train}  Floorplan : {flp} \\n')\n",
        "          if model_name == 'XGBOOST':\n",
        "            model = XGBClassifier().load_model(f'ORIGINAL_{model_name}_eps_0_train_{train}_{flp}.json')\n",
        "          elif model_name == 'CATBOOST':\n",
        "            model = CatBoostClassifier().load_model(f'ORIGINAL_{model_name}_eps_0_train_{train}_{flp}.cbm')\n",
        "          else:\n",
        "            model = tf.keras.models.load_model(f'ORIGINAL_{model_name}_eps_0_train_{train}_{flp}.keras')\n",
        "          if model:\n",
        "            fgsm_x, fgsm_y = train_fgsm(train_x, train_y, eps, model, aug, model_name, train, flp)\n",
        "            fgsm_org_x = np.concatenate((train_x, fgsm_x), axis=0)\n",
        "            fgsm_org_y = np.concatenate((train_y, fgsm_y), axis=0)\n",
        "            print(f'\\n************ Training Model : {aug} {eps} {flp} + {model_name} ************\\n')\n",
        "\n",
        "            # [JJ] [IB] : Add more if conditions for other models\n",
        "            if model_name == 'DNN':\n",
        "              model = train_DNN(fgsm_org_x, fgsm_org_y, 700, batch_size=128)\n",
        "              save_name = f'{aug}_{model_name}_eps_{eps}_train_{train}_{flp}'\n",
        "              model.save(f'{save_name}.keras')\n",
        "              model_params(model, f'{aug}_{model_name}_{train}_eps_{eps}_{flp}')\n",
        "            if model_name == 'CATBOOST':\n",
        "              model = train_Catboost(fgsm_org_x, fgsm_org_y, 50)\n",
        "              save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "              model.save_model(f'{save_name}.cbm')\n",
        "              model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "            if model_name == 'XGBOOST':\n",
        "              model = train_XGboost(fgsm_org_x, fgsm_org_y)\n",
        "              save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "              model.save_model(f'{save_name}.json')\n",
        "              #model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "\n",
        "          else:\n",
        "            print(f'Pre-trained model required for {aug}')\n",
        "        print(f'Training FGSM Complete .... \\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "      if aug == 'PGD':\n",
        "        for eps in epsilon:\n",
        "          print(f'Training on PGD with Epsilon = {eps} - Train Device : {train}  Floorplan : {flp} \\n')\n",
        "          if model_name == 'XGBOOST':\n",
        "            model = XGBClassifier().load_model(f'ORIGINAL_{model_name}_eps_0_train_{train}_{flp}.json')\n",
        "          elif model_name == 'CATBOOST':\n",
        "            model = CatBoostClassifier().load_model(f'ORIGINAL_{model_name}_eps_0_train_{train}_{flp}.cbm')\n",
        "          else:\n",
        "            model = tf.keras.models.load_model(f'ORIGINAL_{model_name}_eps_0_train_{train}_{flp}.keras')\n",
        "          if model:\n",
        "            pgd_x, pgd_y = train_pgd(train_x, train_y, eps, model)\n",
        "            pgd_org_x = np.concatenate((train_x, pgd_x), axis=0)\n",
        "            pgd_org_y = np.concatenate((train_y, pgd_y), axis=0)\n",
        "            print(f'\\n************ Training Model : {aug} {eps} {flp} + {model_name} ************\\n')\n",
        "\n",
        "            # [JJ] [IB] : Add more if conditions for other models\n",
        "            if model_name == 'DNN':\n",
        "              model = train_DNN(pgd_org_x, pgd_org_y, 700, batch_size=128)\n",
        "              save_name = f'{aug}_{model_name}_eps_{eps}_train_{train}_{flp}'\n",
        "              model.save(f'{save_name}.keras')\n",
        "              model_params(model, f'{aug}_{model_name}_{train}_eps_{eps}_{flp}')\n",
        "            if model_name == 'CATBOOST':\n",
        "              model = train_Catboost(pgd_org_x, pgd_org_y, 50)\n",
        "              save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "              model.save_model(f'{save_name}.cbm')\n",
        "              model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "            if model_name == 'XGBOOST':\n",
        "              model = train_XGboost(pgd_org_x, pgd_org_y)\n",
        "              save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "              model.save_model(f'{save_name}.json')\n",
        "              #model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "          else:\n",
        "            print(f'Pre-trained model required for {aug}')\n",
        "        print(f'Training PGD Complete .... \\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "      if aug == 'MIM':\n",
        "        for eps in epsilon:\n",
        "          print(f'Training on MIM with Epsilon = {eps} - Train Device : {train}  Floorplan : {flp} \\n')\n",
        "          if model_name == 'XGBOOST':\n",
        "            model = XGBClassifier().load_model(f'ORIGINAL_{model_name}_eps_0_train_{train}_{flp}.json')\n",
        "          elif model_name == 'CATBOOST':\n",
        "            model = CatBoostClassifier().load_model(f'ORIGINAL_{model_name}_eps_0_train_{train}_{flp}.cbm')\n",
        "          else:\n",
        "            model = tf.keras.models.load_model(f'ORIGINAL_{model_name}_eps_0_train_{train}_{flp}.keras')\n",
        "          if model:\n",
        "            mim_x, mim_y = train_mim(train_x, train_y, eps, model)\n",
        "            mim_org_x = np.concatenate((train_x, mim_x), axis=0)\n",
        "            mim_org_y = np.concatenate((train_y, mim_y), axis=0)\n",
        "            print(f'\\n************ Training Model : {aug} {eps} {flp} + {model_name} ************\\n')\n",
        "\n",
        "            # [JJ] [IB] : Add more if conditions for other models\n",
        "            if model_name == 'DNN':\n",
        "              model = train_DNN(mim_org_x, mim_org_y, 700, batch_size=128)\n",
        "              save_name = f'{aug}_{model_name}_eps_{eps}_train_{train}_{flp}'\n",
        "              model.save(f'{save_name}.keras')\n",
        "              model_params(model, f'{aug}_{model_name}_{train}_eps_{eps}_{flp}')\n",
        "\n",
        "            if model_name == 'CATBOOST':\n",
        "              model = train_Catboost(mim_org_x, mim_org_y, 50)\n",
        "              save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "              model.save_model(f'{save_name}.cbm')\n",
        "              model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "\n",
        "            if model_name == 'XGBOOST':\n",
        "              model = train_XGboost(mim_org_x, mim_org_y)\n",
        "              save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "              model.save_model(f'{save_name}.json')\n",
        "              #model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "\n",
        "            print(f'Training MIM Complete .... \\n')\n",
        "            print('--------------------------------------------------------------------------------------------')\n",
        "            print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "          else:\n",
        "            print(f'Pre-trained model required for {aug}')\n",
        "        print(f'Training MIM Complete .... \\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "      if aug == 'BIM':\n",
        "        for eps in epsilon:\n",
        "          print(f'Training on BIM with Epsilon = {eps} - Train Device : {train}  Floorplan : {flp} \\n')\n",
        "          if model_name == 'XGBOOST':\n",
        "            model = XGBClassifier().load_model(f'ORIGINAL_{model_name}_eps_0_train_{train}_{flp}.json')\n",
        "          elif model_name == 'CATBOOST':\n",
        "            model = CatBoostClassifier().load_model(f'ORIGINAL_{model_name}_eps_0_train_{train}_{flp}.cbm')\n",
        "          else:\n",
        "            model = tf.keras.models.load_model(f'ORIGINAL_{model_name}_eps_0_train_{train}_{flp}.keras')\n",
        "          if model:\n",
        "            bim_x, bim_y = train_bim(train_x, train_y, eps, model)\n",
        "            bim_org_x = np.concatenate((train_x, bim_x), axis=0)\n",
        "            bim_org_y = np.concatenate((train_y, bim_y), axis=0)\n",
        "            print(f'\\n************ Training Model : {aug} {eps} {flp} + {model_name} ************\\n')\n",
        "\n",
        "            # [JJ] [IB] : Add more if conditions for other models\n",
        "            if model_name == 'DNN':\n",
        "              model = train_DNN(bim_org_x, bim_org_y, 700, batch_size=128)\n",
        "              save_name = f'{aug}_{model_name}_eps_{eps}_train_{train}_{flp}'\n",
        "              model.save(f'{save_name}.keras')\n",
        "              model_params(model, f'{aug}_{model_name}_{train}_eps_{eps}_{flp}')\n",
        "            if model_name == 'CATBOOST':\n",
        "              model = train_Catboost(bim_org_x, bim_org_y, 50)\n",
        "              save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "              model.save_model(f'{save_name}.cbm')\n",
        "              model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "            if model_name == 'XGBOOST':\n",
        "              model = train_XGboost(bim_org_x, bim_org_y)\n",
        "              save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "              model.save_model(f'{save_name}.json')\n",
        "              #model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "\n",
        "          else:\n",
        "            print(f'Pre-trained model required for {aug}')\n",
        "        print(f'Training BIM Complete .... \\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "      if aug == 'CW':\n",
        "        for eps in epsilon:\n",
        "          print(f'Training on BIM with Epsilon = {eps} - Train Device : {train}  Floorplan : {flp} \\n')\n",
        "          if model_name == 'XGBOOST':\n",
        "            model = XGBClassifier().load_model(f'ORIGINAL_{model_name}_eps_0_train_{train}_{flp}.json')\n",
        "          elif model_name == 'CATBOOST':\n",
        "            model = CatBoostClassifier().load_model(f'ORIGINAL_{model_name}_eps_0_train_{train}_{flp}.cbm')\n",
        "          else:\n",
        "            model = tf.keras.models.load_model(f'ORIGINAL_{model_name}_eps_0_train_{train}_{flp}.keras')\n",
        "          if model:\n",
        "            cw_x, cw_y = train_cw(train_x, train_y, eps, model)\n",
        "            cw_org_x = np.concatenate((train_x, cw_x), axis=0)\n",
        "            cw_org_y = np.concatenate((train_y, cw_y), axis=0)\n",
        "            print(f'\\n************ Training Model : {aug} {eps} {flp} + {model_name} ************\\n')\n",
        "\n",
        "            # [JJ] [IB] : Add more if conditions for other models\n",
        "            if model_name == 'DNN':\n",
        "              model = train_DNN(cw_org_x, cw_org_y, 700, batch_size=128)\n",
        "              save_name = f'{aug}_{model_name}_eps_{eps}_train_{train}_{flp}'\n",
        "              model.save(f'{save_name}.keras')\n",
        "              model_params(model, f'{aug}_{model_name}_{train}_eps_{eps}_{flp}')\n",
        "            if model_name == 'CATBOOST':\n",
        "              model = train_Catboost(cw_org_x, cw_org_y, 50)\n",
        "              save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "              model.save_model(f'{save_name}.cbm')\n",
        "              model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "            if model_name == 'XGBOOST':\n",
        "              model = train_XGboost(cw_org_x, cw_org_y)\n",
        "              save_name = f'{aug}_{model_name}_eps_0_train_{train}_{flp}'\n",
        "              model.save_model(f'{save_name}.json')\n",
        "              #model_params(model, f'{aug}_{model_name}_{train}_eps_0_{flp}')\n",
        "\n",
        "          else:\n",
        "            print(f'Pre-trained model required for {aug}')\n",
        "        print(f'Training CW Complete .... \\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "zip_filename = f'{model_name}_models.zip'\n",
        "# Create a Zip\n",
        "with ZipFile(zip_filename, 'w') as zipf:\n",
        "    for foldername, subfolders, filenames in os.walk('.'):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith('.keras') or filename.endswith('.txt') or filename.endswith('.cbm') or filename.endswith('.json'):\n",
        "                filepath = os.path.join(foldername, filename)\n",
        "                zipf.write(filepath, os.path.relpath(filepath, start='.'))\n",
        "\n",
        "files.download(zip_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2ugEQzCcSps"
      },
      "outputs": [],
      "source": [
        "#Extra Download cell, for if the above does not finish\n",
        "zip_filename = f'{model_name}_models.zip'\n",
        "# Create a Zip\n",
        "with ZipFile(zip_filename, 'w') as zipf:\n",
        "    for foldername, subfolders, filenames in os.walk('.'):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith('.keras') or filename.endswith('.txt') or filename.endswith('.cbm') or filename.endswith('.json'):\n",
        "                filepath = os.path.join(foldername, filename)\n",
        "                zipf.write(filepath, os.path.relpath(filepath, start='.'))\n",
        "\n",
        "files.download(zip_filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_classes_maker(model, predictions):\n",
        "  if model_name == 'CATBOOST':\n",
        "    prediction_classes = []\n",
        "    for prediction in predictions:\n",
        "      prediction_classes.append(prediction[0])\n",
        "    prediction_classes = np.array(prediction_classes)\n",
        "  else:\n",
        "    prediction_classes = np.argmax(predictions, axis=1)\n",
        "  return prediction_classes"
      ],
      "metadata": {
        "id": "ZG62Rpdw41K0"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrMCDZpd8Eo6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "model_name = 'DNN'\n",
        "# Testing the Trained Models\n",
        "summary_data = []\n",
        "for att in attack:\n",
        "  if att == 'ORIGINAL':\n",
        "    for aug in augmented_data:\n",
        "      for flp in floorplan:\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print(f'Testing on Original RSS\\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        if aug in mode:\n",
        "          train_eps = 0\n",
        "          test_eps = 0.1\n",
        "          try:\n",
        "            if model_name == 'XGBOOST':\n",
        "              print(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.json')\n",
        "              model = XGBClassifier()\n",
        "              model.load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.json')\n",
        "              print(model)\n",
        "            elif model_name == 'CATBOOST':\n",
        "              model = CatBoostClassifier().load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.cbm')\n",
        "            else:\n",
        "              model = tf.keras.models.load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.keras')\n",
        "          except:\n",
        "            print(f'[WARNING] : {aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.keras Model not found!')\n",
        "\n",
        "          for device in dev:\n",
        "            print(f'\\n Test dev -> {device}  flp -> {flp} Model -> {aug} {model_name} Attack -> {att} Epsilon -> N/A \\n')\n",
        "            # Initial CI only !!!\n",
        "            ci_val = 0\n",
        "            train_aps_value = train_ap_dict[f'{train}_{flp}']\n",
        "            test_x, test_y = test_data(ci_val, train_aps_value, device, flp)\n",
        "            predictions = model.predict(test_x)\n",
        "            prediction_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "            mean_error = np.abs(prediction_classes - test_y)\n",
        "            df = pd.DataFrame({'Prediction': prediction_classes, 'Actual': test_y, 'Error': mean_error})\n",
        "\n",
        "\n",
        "            summary_data.append({\n",
        "                'Floorplan': flp,\n",
        "                'Device': device,\n",
        "                'Training Data': aug,\n",
        "                'Training Epsilon': train_eps,\n",
        "                'Attack': att,\n",
        "                'Inference Epsilon': test_eps,\n",
        "                'Mean Error': np.mean(mean_error),\n",
        "                'Median Error': np.median(mean_error),\n",
        "                'Min Error': np.min(mean_error),\n",
        "                'Max Error': np.max(mean_error)\n",
        "            })\n",
        "            print(f'Mean Error : {np.mean(mean_error)} Median Error : {np.median(mean_error)}')\n",
        "\n",
        "            df.to_csv(f'{aug}_{model_name}_train_{train}_{flp}_test_{device}.csv', index=False)\n",
        "\n",
        "          else:\n",
        "            for train_eps in epsilon:\n",
        "              try:\n",
        "                if model_name == 'XGBOOST':\n",
        "                  model = XGBClassifier().load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.json')\n",
        "                elif model_name == 'CATBOOST':\n",
        "                  model = CatBoostClassifier().load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.cbm')\n",
        "                else:\n",
        "                  model = tf.keras.models.load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.keras')\n",
        "                flag = 1\n",
        "              except:\n",
        "                print(f'[WARNING] : {aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.keras Model not found!')\n",
        "                flag = -1\n",
        "\n",
        "              if flag == 1:\n",
        "                for device in dev:\n",
        "                  print(f'\\n Test dev -> {device}  flp -> {flp} Model -> {aug} {model_name} Train Epsilon -> {train_eps} Attack -> {att}\\n')\n",
        "                  # Initial CI only !!!\n",
        "                  ci_val = 0\n",
        "                  train_aps_value = train_ap_dict[f'{train}_{flp}']\n",
        "                  test_x, test_y = test_data(ci_val, train_aps_value, device, flp)\n",
        "                  fgsm_x, fgsm_y = train_fgsm(test_x, test_y, 0, model)\n",
        "                  predictions = model.predict(fgsm_x)\n",
        "                  prediction_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "                  mean_error = np.abs(prediction_classes - test_y)\n",
        "                  df = pd.DataFrame({'Prediction': prediction_classes, 'Actual': test_y, 'Error': mean_error})\n",
        "\n",
        "\n",
        "                  summary_data.append({\n",
        "                      'Floorplan': flp,\n",
        "                      'Device': device,\n",
        "                      'Training Data': aug,\n",
        "                      'Training Epsilon': train_eps,\n",
        "                      'Attack': att,\n",
        "                      'Inference Epsilon': test_eps,\n",
        "                      'Mean Error': np.mean(mean_error),\n",
        "                      'Median Error': np.median(mean_error),\n",
        "                      'Min Error': np.min(mean_error),\n",
        "                      'Max Error': np.max(mean_error)\n",
        "                  })\n",
        "                  print(f'Mean Error : {np.mean(mean_error)} Median Error : {np.median(mean_error)}')\n",
        "\n",
        "                  df.to_csv(f'{aug}_{model_name}_train_{train}_trainEPS_{train_eps}_{flp}_test_{device}.csv', index=False)\n",
        "      # summary.to_csv(f'Summary_{aug}_{model_name}_train_{train}_{flp}_test_{device}.csv', index=False)\n",
        "    print('--------------------------------------------------------------------------------------------')\n",
        "    print(f'Inference Complete .... \\n')\n",
        "    print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "  if att == 'FGSM':\n",
        "    for aug in augmented_data:\n",
        "      for flp in floorplan:\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print(f'Testing on FGSM RSS\\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        for train_eps in epsilon:\n",
        "          try:\n",
        "            if model_name == 'XGBOOST':\n",
        "              model = XGBClassifier().load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.json')\n",
        "            elif model_name == 'CATBOOST':\n",
        "              model = CatBoostClassifier().load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.cbm')\n",
        "            else:\n",
        "              model = tf.keras.models.load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.keras')\n",
        "            flag = 1\n",
        "          except:\n",
        "            print(f'[WARNING] : {aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.keras Model not found!')\n",
        "            flag = -1\n",
        "\n",
        "          if flag == 1:\n",
        "            for device in dev:\n",
        "              for test_eps in epsilon:\n",
        "                print(f'\\n Test dev -> {device}  flp -> {flp} Model -> {aug} {model_name} Train Epsilon -> {train_eps} Attack -> {att} Test Epsilon -> {test_eps} \\n')\n",
        "                # Initial CI only !!!\n",
        "                ci_val = 0\n",
        "                train_aps_value = train_ap_dict[f'{train}_{flp}']\n",
        "                test_x, test_y = test_data(ci_val, train_aps_value, device, flp)\n",
        "                fgsm_x, fgsm_y = train_fgsm(test_x, test_y, test_eps, model, aug, model_name, train, flp)\n",
        "                predictions = model.predict(fgsm_x)\n",
        "                prediction_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "                mean_error = np.abs(prediction_classes - test_y)\n",
        "                df = pd.DataFrame({'Prediction': prediction_classes, 'Actual': test_y, 'Error': mean_error})\n",
        "\n",
        "                summary_data.append({\n",
        "                    'Floorplan': flp,\n",
        "                    'Device': device,\n",
        "                    'Training Data': aug,\n",
        "                    'Training Epsilon': train_eps,\n",
        "                    'Attack': att,\n",
        "                    'Inference Epsilon': test_eps,\n",
        "                    'Mean Error': np.mean(mean_error),\n",
        "                    'Median Error': np.median(mean_error),\n",
        "                    'Min Error': np.min(mean_error),\n",
        "                    'Max Error': np.max(mean_error)\n",
        "                })\n",
        "                print(f'Mean Error : {np.mean(mean_error)} Median Error : {np.median(mean_error)}')\n",
        "\n",
        "                df.to_csv(f'{aug}_{model_name}_train_{train}_trainEPS_{train_eps}_{flp}_test_{device}_testEPS_{test_eps}.csv', index=False)\n",
        "      # summary.to_csv(f'Summary_{aug}_{model_name}_train_{train}_{flp}_test_{device}.csv', index=False)\n",
        "    print('--------------------------------------------------------------------------------------------')\n",
        "    print(f'Inference Complete .... \\n')\n",
        "    print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "  if att == 'PGD':\n",
        "    for aug in augmented_data:\n",
        "      for flp in floorplan:\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        print(f'Testing on PGD RSS\\n')\n",
        "        print('--------------------------------------------------------------------------------------------')\n",
        "        for train_eps in epsilon:\n",
        "          try:\n",
        "            if model_name == 'XGBOOST':\n",
        "              model = XGBClassifier().load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.json')\n",
        "            elif model_name == 'CATBOOST':\n",
        "              model = CatBoostClassifier().load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.cbm')\n",
        "            else:\n",
        "              model = tf.keras.models.load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.keras')\n",
        "            flag = 1\n",
        "          except:\n",
        "            print(f'[WARNING] : {aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.keras Model not found!')\n",
        "            flag = -1\n",
        "\n",
        "          if flag == 1:\n",
        "            for device in dev:\n",
        "              for test_eps in epsilon:\n",
        "                print(f'\\n Test dev -> {device}  flp -> {flp} Model -> {aug} {model_name} Train Epsilon -> {train_eps} Attack -> {att} Test Epsilon -> {test_eps} \\n')\n",
        "                ci_val = 0\n",
        "                train_aps_value = train_ap_dict[f'{train}_{flp}']\n",
        "                test_x, test_y = test_data(ci_val, train_aps_value, device, flp)\n",
        "                pgd_x, pgd_y = train_pgd(test_x, test_y, test_eps, model)\n",
        "                predictions = model.predict(pgd_x)\n",
        "                prediction_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "                mean_error = np.abs(prediction_classes - test_y)\n",
        "                df = pd.DataFrame({'Prediction': prediction_classes, 'Actual': test_y, 'Error': mean_error})\n",
        "\n",
        "                summary_data.append({\n",
        "                    'Floorplan': flp,\n",
        "                    'Device': device,\n",
        "                    'Training Data': aug,\n",
        "                    'Training Epsilon': train_eps,\n",
        "                    'Attack': att,\n",
        "                    'Inference Epsilon': test_eps,\n",
        "                    'Mean Error': np.mean(mean_error),\n",
        "                    'Median Error': np.median(mean_error),\n",
        "                    'Min Error': np.min(mean_error),\n",
        "                    'Max Error': np.max(mean_error)\n",
        "                })\n",
        "                print(f'Mean Error : {np.mean(mean_error)} Median Error : {np.median(mean_error)}')\n",
        "\n",
        "                df.to_csv(f'{aug}_{model_name}_train_{train}_trainEPS_{train_eps}_{flp}_test_{device}_testEPS_{test_eps}.csv', index=False)\n",
        "      # summary.to_csv(f'Summary_{aug}_{model_name}_train_{train}_{flp}_test_{device}.csv', index=False)\n",
        "    print('--------------------------------------------------------------------------------------------')\n",
        "    print(f'Inference Complete .... \\n')\n",
        "    print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "  if att == 'MIM':\n",
        "      for aug in augmented_data:\n",
        "        for flp in floorplan:\n",
        "          print('--------------------------------------------------------------------------------------------')\n",
        "          print(f'Testing on MIM RSS\\n')\n",
        "          print('--------------------------------------------------------------------------------------------')\n",
        "          for train_eps in epsilon:\n",
        "            try:\n",
        "              if model_name == 'XGBOOST':\n",
        "                model = XGBClassifier().load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.json')\n",
        "              elif model_name == 'CATBOOST':\n",
        "                model = CatBoostClassifier().load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.cbm')\n",
        "              else:\n",
        "                model = tf.keras.models.load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.keras')\n",
        "              flag = 1\n",
        "            except:\n",
        "              print(f'[WARNING] : {aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.keras Model not found!')\n",
        "              flag = -1\n",
        "\n",
        "            if flag == 1:\n",
        "              for device in dev:\n",
        "                for test_eps in epsilon:\n",
        "                  print(f'\\n Test dev -> {device}  flp -> {flp} Model -> {aug} {model_name} Train Epsilon -> {train_eps} Attack -> {att} Test Epsilon -> {test_eps} \\n')\n",
        "                  # Initial CI only !!!\n",
        "                  ci_val = 0\n",
        "                  train_aps_value = train_ap_dict[f'{train}_{flp}']\n",
        "                  test_x, test_y = test_data(ci_val, train_aps_value, device, flp)\n",
        "                  mim_x, mim_y = train_mim(test_x, test_y, test_eps, model)\n",
        "                  predictions = model.predict(mim_x)\n",
        "                  prediction_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "                  mean_error = np.abs(prediction_classes - test_y)\n",
        "                  df = pd.DataFrame({'Prediction': prediction_classes, 'Actual': test_y, 'Error': mean_error})\n",
        "\n",
        "                  summary_data.append({\n",
        "                      'Floorplan': flp,\n",
        "                      'Device': device,\n",
        "                      'Training Data': aug,\n",
        "                      'Training Epsilon': train_eps,\n",
        "                      'Attack': att,\n",
        "                      'Inference Epsilon': test_eps,\n",
        "                      'Mean Error': np.mean(mean_error),\n",
        "                      'Median Error': np.median(mean_error),\n",
        "                      'Min Error': np.min(mean_error),\n",
        "                      'Max Error': np.max(mean_error)\n",
        "                  })\n",
        "                  print(f'Mean Error : {np.mean(mean_error)} Median Error : {np.median(mean_error)}')\n",
        "\n",
        "                  df.to_csv(f'{aug}_{model_name}_train_{train}_trainEPS_{train_eps}_{flp}_test_{device}_testEPS_{test_eps}.csv', index=False)\n",
        "        # summary.to_csv(f'Summary_{aug}_{model_name}_train_{train}_{flp}_test_{device}.csv', index=False)\n",
        "      print('--------------------------------------------------------------------------------------------')\n",
        "      print(f'Inference Complete .... \\n')\n",
        "      print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "  if att == 'BIM':\n",
        "      for aug in augmented_data:\n",
        "        for flp in floorplan:\n",
        "          print('--------------------------------------------------------------------------------------------')\n",
        "          print(f'Testing on MIM RSS\\n')\n",
        "          print('--------------------------------------------------------------------------------------------')\n",
        "          for train_eps in epsilon:\n",
        "            try:\n",
        "              if model_name == 'XGBOOST':\n",
        "                model = XGBClassifier().load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.json')\n",
        "              elif model_name == 'CATBOOST':\n",
        "                model = CatBoostClassifier().load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.cbm')\n",
        "              else:\n",
        "                model = tf.keras.models.load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.keras')\n",
        "              flag = 1\n",
        "            except:\n",
        "              print(f'[WARNING] : {aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.keras Model not found!')\n",
        "              flag = -1\n",
        "\n",
        "            if flag == 1:\n",
        "              for device in dev:\n",
        "                for test_eps in epsilon:\n",
        "                  print(f'\\n Test dev -> {device}  flp -> {flp} Model -> {aug} {model_name} Train Epsilon -> {train_eps} Attack -> {att} Test Epsilon -> {test_eps} \\n')\n",
        "                  # Initial CI only !!!\n",
        "                  ci_val = 0\n",
        "                  train_aps_value = train_ap_dict[f'{train}_{flp}']\n",
        "                  test_x, test_y = test_data(ci_val, train_aps_value, device, flp)\n",
        "                  bim_x, bim_y = train_bim(test_x, test_y, test_eps, model)\n",
        "                  predictions = model.predict(bim_x)\n",
        "                  prediction_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "                  mean_error = np.abs(prediction_classes - test_y)\n",
        "                  df = pd.DataFrame({'Prediction': prediction_classes, 'Actual': test_y, 'Error': mean_error})\n",
        "\n",
        "                  summary_data.append({\n",
        "                      'Floorplan': flp,\n",
        "                      'Device': device,\n",
        "                      'Training Data': aug,\n",
        "                      'Training Epsilon': train_eps,\n",
        "                      'Attack': att,\n",
        "                      'Inference Epsilon': test_eps,\n",
        "                      'Mean Error': np.mean(mean_error),\n",
        "                      'Median Error': np.median(mean_error),\n",
        "                      'Min Error': np.min(mean_error),\n",
        "                      'Max Error': np.max(mean_error)\n",
        "                  })\n",
        "                  print(f'Mean Error : {np.mean(mean_error)} Median Error : {np.median(mean_error)}')\n",
        "\n",
        "                  df.to_csv(f'{aug}_{model_name}_train_{train}_trainEPS_{train_eps}_{flp}_test_{device}_testEPS_{test_eps}.csv', index=False)\n",
        "        # summary.to_csv(f'Summary_{aug}_{model_name}_train_{train}_{flp}_test_{device}.csv', index=False)\n",
        "      print('--------------------------------------------------------------------------------------------')\n",
        "      print(f'Inference Complete .... \\n')\n",
        "      print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "  if att == 'CW':\n",
        "      for aug in augmented_data:\n",
        "        for flp in floorplan:\n",
        "          print('--------------------------------------------------------------------------------------------')\n",
        "          print(f'Testing on MIM RSS\\n')\n",
        "          print('--------------------------------------------------------------------------------------------')\n",
        "          for train_eps in epsilon:\n",
        "            try:\n",
        "              if model_name == 'XGBOOST':\n",
        "                model = XGBClassifier().load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.json')\n",
        "              elif model_name == 'CATBOOST':\n",
        "                model = CatBoostClassifier().load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.cbm')\n",
        "              else:\n",
        "                model = tf.keras.models.load_model(f'{aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.keras')\n",
        "              flag = 1\n",
        "            except:\n",
        "              print(f'[WARNING] : {aug}_{model_name}_eps_{train_eps}_train_{train}_{flp}.keras Model not found!')\n",
        "              flag = -1\n",
        "\n",
        "            if flag == 1:\n",
        "              for device in dev:\n",
        "                for test_eps in epsilon:\n",
        "                  print(f'\\n Test dev -> {device}  flp -> {flp} Model -> {aug} {model_name} Train Epsilon -> {train_eps} Attack -> {att} Test Epsilon -> {test_eps} \\n')\n",
        "                  # Initial CI only !!!\n",
        "                  ci_val = 0\n",
        "                  train_aps_value = train_ap_dict[f'{train}_{flp}']\n",
        "                  test_x, test_y = test_data(ci_val, train_aps_value, device, flp)\n",
        "                  cw_x, cw_y = train_cw(test_x, test_y, test_eps, model)\n",
        "                  predictions = model.predict(cw_x)\n",
        "                  prediction_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "                  mean_error = np.abs(prediction_classes - test_y)\n",
        "                  df = pd.DataFrame({'Prediction': prediction_classes, 'Actual': test_y, 'Error': mean_error})\n",
        "\n",
        "                  summary_data.append({\n",
        "                      'Floorplan': flp,\n",
        "                      'Device': device,\n",
        "                      'Training Data': aug,\n",
        "                      'Training Epsilon': train_eps,\n",
        "                      'Attack': att,\n",
        "                      'Inference Epsilon': test_eps,\n",
        "                      'Mean Error': np.mean(mean_error),\n",
        "                      'Median Error': np.median(mean_error),\n",
        "                      'Min Error': np.min(mean_error),\n",
        "                      'Max Error': np.max(mean_error)\n",
        "                  })\n",
        "                  print(f'Mean Error : {np.mean(mean_error)} Median Error : {np.median(mean_error)}')\n",
        "\n",
        "                  df.to_csv(f'{aug}_{model_name}_train_{train}_trainEPS_{train_eps}_{flp}_test_{device}_testEPS_{test_eps}.csv', index=False)\n",
        "        # summary.to_csv(f'Summary_{aug}_{model_name}_train_{train}_{flp}_test_{device}.csv', index=False)\n",
        "      print('--------------------------------------------------------------------------------------------')\n",
        "      print(f'Inference Complete .... \\n')\n",
        "      print('--------------------------------------------------------------------------------------------')\n",
        "\n",
        "summary = pd.DataFrame(summary_data)\n",
        "summary.to_csv(f'Summary_{model_name}.csv', index=False)\n",
        "zip_filename = f'{model_name}_complete.zip'\n",
        "# Create a Zip\n",
        "with ZipFile(zip_filename, 'w') as zipf:\n",
        "    for foldername, subfolders, filenames in os.walk('.'):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith('.keras') or filename.endswith('.txt') or filename.endswith('.csv'):\n",
        "                filepath = os.path.join(foldername, filename)\n",
        "                zipf.write(filepath, os.path.relpath(filepath, start='.'))\n",
        "\n",
        "files.download(zip_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dHeQ1c2J3q3"
      },
      "outputs": [],
      "source": [
        "''' Author - Danish Gufran\n",
        "           - Jadon Johnson\n",
        "           - Isaac Blair\n",
        "           - Sudeep Pasricha\n",
        "\n",
        "EPIC Lab : Colorado State University, Fort Collins\n",
        "\n",
        "<Danish.Gufran@colostate.edu>\n",
        "<Jadon.Johnson@colostate.edu>\n",
        "<Isaac.Blair@colostate.edu>\n",
        "<Sudeep@colostate.edu>\n",
        "\n",
        "Title - Data Augmentation Methods for Wi-Fi RSS Fingerprinting Based Indoor Localization.\n",
        "\n",
        "Citation :\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjrIn6_-6NXI"
      },
      "outputs": [],
      "source": [
        "summary = pd.DataFrame(summary_data)\n",
        "summary.to_csv(f'Summary_{model_name}.csv', index=False)\n",
        "zip_filename = f'{model_name}_complete.zip'\n",
        "# Create a Zip\n",
        "with ZipFile(zip_filename, 'w') as zipf:\n",
        "    for foldername, subfolders, filenames in os.walk('.'):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith('.keras') or filename.endswith('.txt') or filename.endswith('.csv'):\n",
        "                filepath = os.path.join(foldername, filename)\n",
        "                zipf.write(filepath, os.path.relpath(filepath, start='.'))\n",
        "\n",
        "files.download(zip_filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!mkdir dnn_stuff\n",
        "#!mv DNN_complete.zip dnn_stuff\n",
        "!mv Summary_DNN_complete.csv dnn_stuff\n",
        "!mv DNN_models.zip dnn_stuff"
      ],
      "metadata": {
        "id": "XmGBx91Vvfg_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}